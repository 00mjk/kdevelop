<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML
><HEAD
><TITLE
>Standard Interfaces</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.54"/><LINK
REL="HOME"
TITLE="KDE 2.0 Development"
HREF="index.html"/><LINK
REL="UP"
TITLE="Multimedia"
HREF="ch14.html"/><LINK
REL="PREVIOUS"
TITLE="MCOP"
HREF="ch14lev1sec3.html"/><LINK
REL="NEXT"
TITLE="Implementing a StereoEffect"
HREF="ch14lev1sec5.html"/><META
HTTP-EQUIV="Content-Style-Type"
CONTENT="text/css"/><LINK
REL="stylesheet"
HREF="kde-common.css"
TYPE="text/css"/><META
HTTP-EQUIV="Content-Type"
CONTENT="text/html; charset=iso-8859-1"/><META
HTTP-EQUIV="Content-Language"
CONTENT="en"/><LINK
REL="stylesheet"
HREF="kde-localised.css"
TYPE="text/css"
TITLE="KDE-English"/><LINK
REL="stylesheet"
HREF="kde-default.css"
TYPE="text/css"
TITLE="KDE-Default"/></HEAD
><BODY
CLASS="section"
LINK="#336699"
VLINK="#336699"
ALINK="#336699"
BGCOLOR="#FFFFFF"
><DIV
ALIGN="RIGHT"
CLASS="NAVBAR"
><P
><A
HREF="ch14lev1sec3.html"
>Prev</A
> <A
HREF="ch14lev1sec5.html"
>Next</A
> <A
HREF="index.html"
>Table of Contents</A
></P
></DIV
><DIV
CLASS="section"
><TABLE
WIDTH="100%"
CELLPADDING="0"
CELLSPACING="0"
BORDER="0"
ALIGN="CENTER"
><TR
><TD
WIDTH="90%"
><H1
CLASS="section"
><A
NAME="ch14lev1sec4"
>14.4. Standard Interfaces</A
></H1
><P
>The whole point of a middleware such as MCOP is to make objects talk to each other to fulfill their task. The following are some of the interfaces that are the most important to get started.</P
><DIV
CLASS="section"
><TABLE
WIDTH="100%"
CELLPADDING="0"
CELLSPACING="0"
BORDER="0"
ALIGN="CENTER"
><TR
><TD
WIDTH="90%"
><H2
CLASS="section"
><A
NAME="ch14lev2sec16"
>14.4.1. The <TT
CLASS="literal"
>SimpleSoundServer</TT
> Interface</A
></H2
><P
>The <TT
CLASS="literal"
>SimpleSoundServer</TT
> interface is the interface that the KDE soundserver <TT
CLASS="literal"
>artsd</TT
> provides when running. To connect to it, you can simply use the following lines:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;SimpleSoundServer server(
   3&nbsp;    Reference("global:Arts_SimpleSoundServer"));
   4&nbsp;
   5&nbsp;if(server.isNull()) { /* error handling */ }
   6&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>Make sure not to access functions of the server after you find out <TT
CLASS="literal"
>isNull()</TT
> is true. So what does it offer? First, it offers the most basic command, playing some file (which may be .wav or any other format aRts can understand), with the simple method:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;long play(string filename);
   3&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>Therefore, in a few lines, you can write a client that plays wave files. If you already have a <TT
CLASS="literal"
>SimpleSoundServer</TT
> called server, its just</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;server.play("/var/share/sounds/asound.wav");
   3&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><DIV
CLASS="note"
><TABLE
CLASS="note"
WIDTH="100%"
BORDER="0"
><TR
><TD
WIDTH="25"
ALIGN="CENTER"
VALIGN="TOP"
><IMG
SRC="note.png"
HSPACE="5"
ALT="Note"/></TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
><P
>It is necessary here to pass a <I
CLASS="emphasis"
>full path</I
>, because it is very likely that your program doesn't have the same working directory as <TT
CLASS="literal"
>artsd</TT
>. Thus, calling play with an unqualified name will mostly fail. For instance, if you are in /var/share/sounds and <TT
CLASS="literal"
>artsd</TT
> is in /home/kde2, if you write <TT
CLASS="literal"
>play</TT
> (<TT
CLASS="literal"
>"asound.wav"</TT
>), the server would try to play /home/kde2/asound.wav.</P
></TD
></TR
></TABLE
></DIV
><P
>The <TT
CLASS="literal"
>play</TT
> method returns a long value with an ID. If playing succeeded, you can use this to stop the sound again with</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;void stop(long ID);
   3&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>if not, the ID is 0.</P
><P
>Then, there is another set of methods to attach or detach streaming-sound sources, such as games that do their own sound mixing (Quake, for instance). They are called</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;void attach(ByteSoundProducer producer);
   3&nbsp;void detach(ByteSoundProducer producer);
   4&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>If you want to use these, the way to go is to implement a <TT
CLASS="literal"
>ByteSoundProducer</TT
> object. This has an outgoing asynchronous byte stream, which can be used to send the data as signed 16-bit little endian stereo. Then, simply create such an object inside your process. For adapting Quake, the <TT
CLASS="literal"
>ByteSoundProducer</TT
> object should be created inside the Quake process, and all audio output should be put into the data packets sent via the asynchronous streaming mechanism. Finally, a call to <TT
CLASS="literal"
>attach()</TT
> with the object is enough to start streaming.</P
><P
>When you're done, call <TT
CLASS="literal"
>detach()</TT
>. An example showing how to implement a <TT
CLASS="literal"
>ByteSoundProducer</TT
> is in the kdelibs/arts/examples directory. But in most cases, a simpler way is possible. For porting games such as Quake, there is also the C API, which encapsulates the aRts functionality. Thus, there are routines similar to those needed to access the operating system audio drivers, like OSS (open sound system, the Linux sound drivers). These are called <TT
CLASS="literal"
>arts_open()</TT
>, <TT
CLASS="literal"
>arts_write()</TT
>, <TT
CLASS="literal"
>arts_close(),</TT
> and so on, which, in turn, call the things that ought to happen in the background.</P
><P
>Whether a layer will be written to simplify the usage of the streaming API for KDE 2.0 apps remains to be seen. If there is time to do a <TT
CLASS="literal"
>KAudioStream</TT
>, which handles all attach/detach and packet production, it will go into some KDE library.</P
><P
>Finally, two functions are left. One is</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;object createObject(string name);
   3&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>It can be used to create an arbitrary object on the soundserver. Therefore, if you need an <TT
CLASS="literal"
>Example_ADD</TT
> for some reason&#8212;and it shouldn't be running inside your process, but inside the soundserver process&#8212;a call looking like this:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;Example_ADD e = DynamicCast(server.createObject("Example_ADD"));
   3&nbsp;if(e.isNull()) { /* fail */ }
   4&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>should do the trick. As you see, you can easily cast <TT
CLASS="literal"
>Object</TT
> to <TT
CLASS="literal"
>Example_ADD</TT
> using <TT
CLASS="literal"
>DynamicCast</TT
>.</P
><P
>Just a few words explaining why you may want to create something on the server. Imagine that you want to develop a 3D game, but you are missing 3D capabilities inside aRts, such as creating moving sound sources and things like that. Of course, you can render all that locally (inside the game process) and transfer the result via streaming to the soundserver. However, a latency penalty and a performance penalty are associated with that.</P
><P
>The latency penalty is this: you need to do streaming in packets, which have a certain size. If you want to have no dropouts when your game doesn't get the CPU for a few milliseconds, you need to dimension these like four packets with 2048 bytes each, or something like that. Although the resulting total time needed to replay all packets of 47 milliseconds protects you from dropouts, it also means that after a player shoots, you'll have a 47-millisecond delay until the 3D sound system reacts. On the other hand, if your 3D sound system runs inside the server, the time to tell it <SPAN
CLASS="QUOTE"
>"player shoots now"</SPAN
> would normally be around 1 millisecond (because it is one <TT
CLASS="literal"
>oneway</TT
> remote invocation). Thus, you can reduce the latency by 47 milliseconds by creating things server side.</P
><P
>The performance penalty, on the other hand, is clear. Putting all that stuff into packets and taking it out again takes CPU time. With very small latencies (small packets), you need more packets per second, and thus, the performance penalty increases. So for real-time applications such as games, running things server side is the most important.</P
><P
>Last but not least, let's take a look at effects. The server allows inserting effects between the downmixed signal of all clients and the output. That is possible with the attribute</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;readonly attribute StereoEffectStack outstack;
   3&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>As you see, you get a <TT
CLASS="literal"
>StereoEffectStack</TT
>, for which the interface will be described soon. It can be used to add effects to the chain.</P
></TD
><TD
WIDTH="10%"
VALIGN="BOTTOM"
ALIGN="CENTER"
><ANNMARK
NAME="ch14lev2sec16"/></TD
></TR
><ANNOTATION
NAME="ch14lev2sec16"
TITLE="The SimpleSoundServer Interface"/></TABLE
></DIV
><DIV
CLASS="section"
><TABLE
WIDTH="100%"
CELLPADDING="0"
CELLSPACING="0"
BORDER="0"
ALIGN="CENTER"
><TR
><TD
WIDTH="90%"
><H2
CLASS="section"
><A
NAME="ch14lev2sec17"
>14.4.2. The KMedia2 Interfaces</A
></H2
><P
>KMedia2 is nothing but a big remote control. It allows you to create objects that play some kind of media (such as .wavs, MP3s, but&#8212;at least from what the interfaces allow&#8212;also CDs or streams from URLs). This is achieved through one interface, called <TT
CLASS="literal"
>PlayObject</TT
>, and it looks like the following:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;interface PlayObject : PlayObject_private {
   3&nbsp;    attribute string description;
   4&nbsp;    attribute poTime currentTime;
   5&nbsp;    readonly attribute poTime overallTime;
   6&nbsp;    readonly attribute poCapabilities capabilities;
   7&nbsp;    readonly attribute string mediaName;
   8&nbsp;    readonly attribute poState state;
   9&nbsp;    void play();
  10&nbsp;    void seek(poTime newTime);
  11&nbsp;    void pause();
  12&nbsp;};
  13&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>As you can see, this is enough for telling the object to play, pause, and seek anytime. After you have a <TT
CLASS="literal"
>PlayObject</TT
>, you should have no difficulties dealing with it. There is something to be said about the <TT
CLASS="literal"
>poTime</TT
> type, which is used to represent custom times. For instance, a mod player could count in patterns internally, while also doing calculations in seconds (which is more appropriate for the user to read). Thus, <TT
CLASS="literal"
>poTime</TT
> allows you to define custom times, like this:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;struct poTime {
   3&nbsp;    long ms, seconds;  // -1 if undefined
   4&nbsp;    float custom;      // some custom time unit
   5&nbsp;                       // -1 if undefined
   6&nbsp;    string customUnit; // for instance "pattern"
   7&nbsp;};
   8&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
><TT
CLASS="literal"
>PlayObjects</TT
> are allowed to define either the <SPAN
CLASS="QUOTE"
>"normal"</SPAN
> time, the <SPAN
CLASS="QUOTE"
>"custom"</SPAN
> time, or both, just as they please. Also, seeking can be done only on the time type the <TT
CLASS="literal"
>PlayObject</TT
> understands. (For example, if a mod player understands only patterns, you can seek only with patterns). Then there are capabilities, which can be used for the PlayObject to say, <SPAN
CLASS="QUOTE"
>"Well, I am a stream from an URL; you can't seek me at all."</SPAN
> They look like this:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;enum poCapabilities { capSeek = 1, capPause = 2 };
   3&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>and finally the different states the <TT
CLASS="literal"
>PlayObject</TT
> are:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;enum poState { posPlaying, posFinished, posPaused };
   3&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>Still, some part is missing. You not only need to know how to talk to <TT
CLASS="literal"
>PlayObjects</TT
>, you need to know how to create them in the first place. For that, there is <TT
CLASS="literal"
>PlayObjectFactory</TT
>, which looks like the following:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;interface PlayObjectFactory {
   3&nbsp;    PlayObject createPlayObject(string filename);
   4&nbsp;};
   5&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>That's it. You have a factory for creating <TT
CLASS="literal"
>PlayObjects</TT
>, and you know that they will disappear automatically, as soon as you no longer reference them. And the last missing piece, <SPAN
CLASS="QUOTE"
>"Where do I get that <TT
CLASS="literal"
>PlayObjectFactory</TT
> from?"</SPAN
> is simple: it's a global reference, and it is called <TT
CLASS="literal"
>Arts_PlayObjectFactory</TT
>, so it works the same as with the <TT
CLASS="literal"
>SimpleSoundServer</TT
> interface.</P
></TD
><TD
WIDTH="10%"
VALIGN="BOTTOM"
ALIGN="CENTER"
><ANNMARK
NAME="ch14lev2sec17"/></TD
></TR
><ANNOTATION
NAME="ch14lev2sec17"
TITLE="The KMedia2 Interfaces"/></TABLE
></DIV
><DIV
CLASS="section"
><TABLE
WIDTH="100%"
CELLPADDING="0"
CELLSPACING="0"
BORDER="0"
ALIGN="CENTER"
><TR
><TD
WIDTH="90%"
><H2
CLASS="section"
><A
NAME="ch14lev2sec18"
>14.4.3. Stereo Effects/Effectstacks</A
></H2
><P
>Let's take a closer look at another interface that is used to put effects in a chain. Basically, a <TT
CLASS="literal"
>StereoEffectStack</TT
> looks like <A
HREF="ch14lev1sec4.html#ch14fig06"
>Figure 14.6</A
>:</P
><DIV
CLASS="figure"
><HR/><A
NAME="ch14fig06"
></A
><P
><B
>Figure 14.6. How a <TT
CLASS="literal"
>StereoEffectStack</TT
> works.</B
></P
><DIV
CLASS="mediaobject"
><P
><IMG
SRC="graphics/14fig06.gif"
></IMG
></P
></DIV
><HR/></DIV
><P
>Each of the inserted effects should have the following interface:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;interface StereoEffect : SynthModule {
   3&nbsp;    default in audio stream inleft, inright;
   4&nbsp;    default out audio stream outleft, outright;
   5&nbsp;};
   6&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>and all normal <TT
CLASS="literal"
>StereoEffects</TT
> derive from that. For example, two of them are <TT
CLASS="literal"
>StereoVolumeControl</TT
> and <TT
CLASS="literal"
>StereoFFTScope</TT
>. However, there is the <TT
CLASS="literal"
>StereoEffectStack</TT
> interface itself, which looks like this:</P
><DIV
CLASS="informalexample"
><HR/><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="programlisting"
>   1&nbsp;
   2&nbsp;interface StereoEffectStack : StereoEffect {
   3&nbsp;    long insertTop(StereoEffect effect, string name);
   4&nbsp;    long insertBottom(StereoEffect effect, string name);
   5&nbsp;
   6&nbsp;    void remove(long ID);
   7&nbsp;};
   8&nbsp;</PRE
></TD
></TR
></TABLE
><HR/></DIV
><P
>As you can see, the <TT
CLASS="literal"
>StereoEffectStack</TT
> is a <TT
CLASS="literal"
>StereoEffect</TT
> itself, which means it has the same inleft, inright, outleft, and outright streams. Thus, you can set the inputs and outputs by connecting these. You can also insert effects at the top or at the bottom. If you have no effects, the inputs and outputs will simply get connected. Finally, you can remove effects again by ID.</P
><P
><TT
CLASS="literal"
>SimpleSoundServer</TT
> provides you with a <TT
CLASS="literal"
>StereoEffectStack</TT
> that is between the sound mixing and the output. Initially, it's empty. If you want effects, you can insert them into the stack, and if you want to remove them, that's also no issue.</P
><P
>And that is what you're going to do next.</P
></TD
><TD
WIDTH="10%"
VALIGN="BOTTOM"
ALIGN="CENTER"
><ANNMARK
NAME="ch14lev2sec18"/></TD
></TR
><ANNOTATION
NAME="ch14lev2sec18"
TITLE="Stereo Effects/Effectstacks"/></TABLE
></DIV
></TD
><TD
WIDTH="10%"
VALIGN="BOTTOM"
ALIGN="CENTER"
><ANNMARK
NAME="ch14lev1sec4"/></TD
></TR
><ANNOTATION
NAME="ch14lev1sec4"
TITLE="Standard Interfaces"/></TABLE
></DIV
><DIV
ALIGN="RIGHT"
CLASS="NAVBAR"
><P
><A
HREF="ch14lev1sec3.html"
>Prev</A
> <A
HREF="ch14lev1sec5.html"
>Next</A
> <A
HREF="index.html"
>Table of Contents</A
></P
></DIV
><HR
WIDTH="100%"
SIZE="2"
ALIGN="CENTER"
NOSHADE="NOSHADE"/></BODY
></HTML
>