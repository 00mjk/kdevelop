<chapter label="14" role="chapter" id="ch14">
<title>Multimedia</title>
<para><emphasis>by Stefan Westerfeld</emphasis></para>
<highlights>
<itemizedlist mark="bullet" spacing="compact">
<title>In This Chapter</title>
<listitem><para><emphasis role="strong"><link linkend="ch14lev1sec1">Introducing aRts/MCOP</link></emphasis></para></listitem>
<listitem><para><emphasis role="strong"><link linkend="ch14lev1sec2">A First Glance at Writing Modules</link></emphasis></para></listitem>
<listitem><para><emphasis role="strong"><link linkend="ch14lev1sec3">MCOP</link></emphasis></para></listitem>
<listitem><para><emphasis role="strong"><link linkend="ch14lev1sec4">Standard Interfaces</link></emphasis></para></listitem>
<listitem><para><link linkend="ch14lev1sec5"><emphasis role="strong">Implementing a</emphasis> <literal>SteroEffect</literal></link></para></listitem>
<listitem><para><emphasis role="strong"><link linkend="ch14lev1sec6">KDE Multimedia Besides MCOP</link></emphasis></para></listitem>
<listitem><para><emphasis role="strong"><link linkend="ch14lev1sec7">The Future of MCOP</link></emphasis></para></listitem>
</itemizedlist>
<para>What<indexterm><primary>multimedia</primary></indexterm> has traditionally been the domain of other systems is slowly coming to Linux (and UNIX) desktops. Images, sound effects, music, and video are a fascinating way to make applications more lively and to enable whole new uses. When I was showing a KDE 2.0 preview at the CeBIT 2000, I often presented some of the multimedia stuff&mdash;nice sound effects, flashing lights, and great music. Many people who were passing by stopped and could not take their eyes off the screen. Multimedia programs capture much more attention from a user than simple, <quote>boring</quote> applications that just run in a rectangular space and remain silent and unmoving.</para>
<para>However, those technologies will become widespread in KDE applications only if they are easily accessible for developers. Take audio as an example. KDE is supposed to run on a variety of UNIX platforms, and not all of them support sound. Among those systems that do, there are very different ways of accessing the sound driver. Writing a proper (portable) application isn't really easy.</para>
<para>KDE 1.0 started providing support for playing sound effects easily with the KAudioServer. Thus, a game such as KReversi could support sound without caring about portability. Using one <literal>KAudio</literal> class, all problems regarding different platforms, and how exactly to load, decode, and play such a file, were gone.</para>
<para>The idea of KDE 2.0 multimedia support remains the same: make multimedia technologies easily accessible to developers. It is the dimension that changed. For KDE 1.0, playing a wave file was about all the multimedia support you could get from the libraries. For KDE 2.0 and beyond, the idea is to really care about multimedia.</para>
<para>KDE 2.0 takes into consideration all audio applications&mdash;not only those that casually play a file, but everything from the heavy real-time-oriented game to the sequencer. KDE 2.0 also supports plug-ins and small modules that can easily be recombined, as well as MIDI support and video support.</para>
<para>The challenge of delivering multimedia in all forms to the KDE desktop is big. Thus, the KDE multimedia support should work like a glue between the applications so that the puzzle pieces already solved by various programmers will be usable in any of the applications, and the image will slowly grow complete.<indexterm><primary>multimedia</primary></indexterm></para>
</highlights>
<section id="ch14lev1sec1">
<title>Introducing aRts/MCOP</title>
<para>The <indexterm><primary>multimedia</primary><secondary>aRts (analog, real-time synthesis)</secondary></indexterm><indexterm><primary>aRts (analog, real-time synthesis)</primary></indexterm><indexterm><primary>analog, real-time synthesis (aRts)</primary></indexterm>road for KDE 2.0 (and later versions) is integration through one consistent streaming-media technology. The idea is that you can write any multimedia task as little pieces, which pass multimedia streams.</para>
<para>Quite some time before KDE 2.0, I first heard of the plans of KAudioServer2 (from Christian Esken), which was an attempt to improve and rewrite the audioserver to support streaming media to a certain degree. On the other hand, I had been working on aRts (analog, real-time synthesis) software for quite a while and had already implemented some nice streaming support. In fact, aRts was a modular software synthesizer that worked through little plug-ins and streams between them. And, most important of all, aRts was already working great.</para>
<para>So, after some considerations, we decided at the KDE 2.0 Developer meeting to make aRts the base for all streaming multimedia under KDE. Many things would have to be changed to come from one synthesizer to a base for all multimedia tasks, but it was the much better approach than trying to do something completely new and different, because aRts was already proven to work.</para>
<para>As I see it, the important parts of streaming multimedia support are</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem><para>An easy way to write small modules, which can be used for streaming (plug-ins).</para></listitem>
<listitem><para>A way to define how these modules communicate (what types of data they accept, what properties they have, what functions they support).</para></listitem>
<listitem><para>A scheduler that decides what module gets executed when&mdash;this is necessary because you usually have lots of small modules running in one task.</para></listitem>
<listitem><para>A transfer layer, which ensures that modules running in different processes/applications or on different computers can communicate.</para></listitem>
</itemizedlist>
<para>How these things work is probably illustrated best with a small example. Assume you want to listen to a beep while the left speaker should be playing a 440Hz frequency and the right speaker is playing a 880Hz frequency. That would look something like the following:<indexterm><primary>multimedia</primary><secondary>aRts (analog, real-time synthesis)</secondary></indexterm><indexterm><primary>aRts (analog, real-time synthesis)</primary></indexterm><indexterm><primary>analog, real-time synthesis (aRts)</primary></indexterm></para>
<figure label="14.1" id="ch14fig01">
<title>The flow graph of a stereo beep.</title>
<mediaobject>
<imageobject>
<imagedata fileref="graphics/14fig01.gif" format="GIF"/>
</imageobject>
</mediaobject>
</figure>
<para>As you see, the task has been divided into very small components, each of which do only a part of the whole. The frequency generators only generate the frequency (they can also be used for other wave forms), nothing more. The sine wave objects only calculate the sinus of the values they get. The play object only takes care that these things really reach your sound card. To get a first impression, the source code for this example is shown in <link linkend="ch14list01">Listing 14.1</link>:<indexterm><primary>multimedia</primary><secondary>aRts (analog, real-time synthesis)</secondary></indexterm><indexterm><primary>aRts (analog, real-time synthesis)</primary></indexterm><indexterm><primary>analog, real-time synthesis (aRts)</primary></indexterm></para>
<example role="codelisting" label="14.1" id="ch14list01">
<title>Listening to a Stereo Beep<indexterm><primary>multimedia</primary><secondary>stero beeps, playing</secondary></indexterm><indexterm><primary>stereo beeps, playing</primary></indexterm><indexterm><primary>sound</primary><secondary>stereo beeps, playing</secondary></indexterm><indexterm><primary>playing</primary><secondary>stero beeps</secondary></indexterm><indexterm><primary>listings</primary><secondary>stereo beep, playing</secondary></indexterm></title>
<programlisting linenumbering="numbered">
 1: // first_example.cc
 2:
 3: #include "artsflow.h"
 4: #include "connect.h"
 5:
 6: using namespace Arts;
 7:
 8: int main()
 9: {
10:     Dispatcher dispatcher;
11:
12:     Synth_FREQUENCY freq1,freq2;   // object creation
13:     Synth_WAVE_SIN  sin1,sin2;
14:     Synth_PLAY      play;
15:
16:     setValue(freq1, 440.0);       // set frequencies
17:     setValue(freq2, 880.0);
18:
19:     connect(freq1, sin1);         // object connection
20:     connect(freq2, sin2);
21:     connect(sin1, play, "invalue_left");
22:     connect(sin2, play, "invalue_right");
23:
24:     freq1.start(); freq2.start();  // start&amp;go
25:     sin1.start(); sin2.start();
26:     play.start();
27:     dispatcher.run();
28: }<indexterm><primary>multimedia</primary><secondary>stero beeps, playing</secondary></indexterm><indexterm><primary>stereo beeps, playing</primary></indexterm><indexterm><primary>sound</primary><secondary>stereo beeps, playing</secondary></indexterm><indexterm><primary>playing</primary><secondary>stero beeps</secondary></indexterm><indexterm><primary>listings</primary><secondary>stereo beep, playing</secondary></indexterm>
</programlisting>
</example>
<para>Now, while you're thinking of that simplistic example, consider <link linkend="ch14fig02">Figure 14.2</link>:<indexterm><primary>multimedia</primary><secondary>aRts (analog, real-time synthesis)</secondary></indexterm><indexterm><primary>aRts (analog, real-time synthesis)</primary></indexterm><indexterm><primary>analog, real-time synthesis (aRts)</primary></indexterm></para>
<para><link linkend="ch14fig02">Figure 14.2</link> illustrates a real-life example. I've simply composed three tasks done at the same time.</para>
<para>First, consider the MIDI player. The MIDI-player component is probably reading a file and sending out MIDI events. These are sent through a software synthesizer, which takes the incoming MIDI events and converts them to an audio stream. This is not about your hardware wave table on the sound card; all things that we are talking about here are happening before the data is sent to the sound card.</para>
<para>On the other hand, there is the game. Games often have very specific requirements for how they calculate their sound, so they might have a complete engine that does this task. One example is Quake. It calculates sound effects according to the player's position, so you can orient yourself by listening closely to what you hear. In that case, the game generates a complete audio stream itself, which only will be sent to the mixer.</para>
<figure label="14.2" id="ch14fig02">
<title>A flow graph of some real-life applications running.</title>
<mediaobject>
<imageobject>
<imagedata fileref="graphics/14fig02.gif" format="GIF"/>
</imageobject>
</mediaobject>
</figure>
<para>The next chain is the one with the microphone attached. The microphone output is sent through a pitch-shifting effect in this example. Then the output goes through the mixer, the same as everything else. Through the pitch shifting, your voice sounds higher (or lower) because the frequency changes give this a funny cartoon-character effect. If you like, you can also imagine a more <quote>serious</quote> application, such as speech recognition or Internet telephony at this place.<indexterm><primary>multimedia</primary><secondary>aRts (analog, real-time synthesis)</secondary></indexterm><indexterm><primary>aRts (analog, real-time synthesis)</primary></indexterm><indexterm><primary>analog, real-time synthesis (aRts)</primary></indexterm></para>
<para>Finally, everything is mixed in the mixer component, and then, after sending it through a last effect (which adds the reverb effect), played over your sound card.</para>
<para>This example shows a bit more of what the multimedia support does here. You, for instance, see that not all components that are involved are in the same process. You wouldn't want to run your Quake game inside the audioserver, which also does the other tasks. Maybe your MIDI player is also external; maybe it is a component that runs inside the audioserver. Thus, the signal flow is distributed between the processes. The components that are responsible for certain tasks run where it fits best.</para>
<para>You also see that different kinds of streams are involved. The first is normal audio streams, which are managed nicely by the aRts/MCOP combination (and the most convenient method). The second is the MIDI stream. These differ a lot. An audio stream always carries data. In one second, 44,100 values are passed across the stream. In contrast, a MIDI stream transmits something only when it is needed. When a note is played, a small event is sent over the stream; when nothing happens, nothing is sent.</para>
<para>The third type is byte audio, which refers to the way the game in that case could produce audio. Byte stream is the same format that would normally be replayed through the sound card (16 bit, little endian, 44kHz, stereo). To process such data with the mixer, it needs to go through a converter because the mixer only mixes <quote>real</quote> audio streams.<indexterm><primary>multimedia</primary><secondary>aRts (analog, real-time synthesis)</secondary></indexterm><indexterm><primary>aRts (analog, real-time synthesis)</primary></indexterm><indexterm><primary>analog, real-time synthesis (aRts)</primary></indexterm></para>
<section id="ch14lev2sec1">
<title>Overview of This Chapter</title>
<para>For<indexterm><primary>multimedia</primary></indexterm> two reasons, most of this chapter is about aRts/MCOP: One is that I know it very well because I wrote most of the code. The other is that I think it is the most essential part of the KDE 2.0 multimedia strategy and will provide a way to get to one unified standard for all multimedia tasks.</para>
<para>I start with a practical example: how to write a small module, as I mentioned in the section <link linkend="ch14lev1sec1"><quote>Introducing aRts/MCOP,</quote></link> and how to use it. You'll get an impression of how it works.</para>
<para>Then I give more background about MCOP, the CORBA-like middleware that is the base for all multimedia tasks. In the section <link linkend="ch14lev1sec3"><quote>MCOP,</quote></link> I write specifically about how MCOP enables objects to do streaming in a very natural way.</para>
<para>But MCOP is nothing when there are no interfaces to talk to. In the sections <link linkend="ch14lev1sec4"><quote>Standard Interfaces</quote></link> and <link linkend="ch14lev1sec5"><quote>Implementing a <literal>StereoEffect</literal>,</quote></link> you see the standard interfaces that come with KDE 2.0 and why they exist. Then you'll transform the simple example into a stereo effect.</para>
<para>After that, I explain a few things about other multimedia facilities that KDE offers, which are not MCOP based. For those of you who don't want to get deep into multimedia, but just have your mail application play a <quote>pling</quote> when mail arrives, this may be the thing that interests you most.</para>
<para>Finally, this chapter ends with a view about the future. Where are we going? What are the possibilities that should be available in further versions of KDE? What can you work on when you are interested in actually improving KDE multimedia support?<indexterm><primary>multimedia</primary></indexterm></para>
</section>
</section>
<section id="ch14lev1sec2">
<title>A First Glance at Writing Modules</title>
<para>The <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary></indexterm><indexterm><primary>modules (MCOP), writing</primary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary></indexterm>way you work when writing MCOP-aware objects is normally the following:</para>
<orderedlist numeration="arabic" continuation="restarts" spacing="compact">
<listitem><para>Write an interface definition in the IDL language; for instance, example_add.idl.</para></listitem>
<listitem><para>Pass that definition through <literal>mcopidl.</literal> You get example_add.cc and example_add.h files.</para></listitem>
<listitem><para>Write an implementation for the interfaces you've declared, as C++ class deriving from the <literal>_skel</literal> classes.</para></listitem>
<listitem><para>Register that implementation with <literal>REGISTER_IMPLEMENTATION.</literal></para></listitem>
<listitem><para>Maybe write a .mcopclass file.</para></listitem>
</orderedlist>
<para>After that, everybody can use the things you do.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary></indexterm><indexterm><primary>modules (MCOP), writing</primary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary></indexterm></para>
<section id="ch14lev2sec2">
<title>Step 1&mdash;Write an Interface Definition in the IDL Language</title>
<para>One<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>interface definitions</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>interface definitions</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>interface definitions</tertiary></indexterm> important concept in MCOP is that classes are not important, interfaces are. To show a simple example, when you write a small module that simply adds two audio streams, it could have the following interface:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
// example_add.idl

#include &lt;artsflow.idl>

interface Example_ADD : Arts::SynthModule {
    in audio stream invalue1, invalue2;
    out audio stream result;
};
</programlisting>
</informalexample>
<para>You describe interfaces like that in the MCOP IDL files. These lines mean: there is an interface in which two audio streams are flowing in, and one audio stream is flowing out.</para>
<figure label="14.3" id="ch14fig03">
<title>The <literal>Example_ADD</literal> interface.</title>
<mediaobject>
<imageobject>
<imagedata fileref="graphics/14fig03.gif" format="GIF"/>
</imageobject>
</mediaobject>
</figure>
<para>For people who use this interface, this is all they need to know. They don't need to know how addition takes place. They don't need to know what language this was implemented in. They don't need to know anything except the definitions in the interface.</para>
<para>Let's do a line-by-line walk-through to see what is happening here:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
#include &lt;artsflow.idl>
</programlisting>
</informalexample>
<para>Because the <literal>SynthModule</literal> interface (which you use later) is declared in artsflow.idl, you need to include it. All aRts components are declared inside the <literal>Arts</literal> namespace, so you have to prefix it with <literal>Arts::</literal>. I'll never explicitly mention this prefix in the text when discussing interfaces.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>interface definitions</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>interface definitions</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>interface definitions</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface Example_ADD : Arts::SynthModule
</programlisting>
</informalexample>
<para>This tells the MCOP IDL compiler to create an interface that implements everything that <literal>SynthModule</literal> does, as well as its own methods, attributes, and streams. (So it derives from <literal>SynthModule.</literal>) MCOP supports multiple inheritance as well as single inheritance. Interfaces that don't specify anything automatically derive from <literal>Object</literal>. Interfaces that have streams (like our interface) should always inherit <literal>SynthModule</literal> (or a derived class).</para>
<informalexample>
<programlisting linenumbering="unnumbered">
in audio stream invalue1, invalue2;
out audio stream outvalue;
</programlisting>
</informalexample>
<para>Here you add streams to the interface. These streams are the normal type of audio stream supported by MCOP. They are <emphasis>synchronous</emphasis>, which means that every time our <literal>Example_ADD</literal> module gets 200 samples (or any other amount), all streams are involved. The scheduler takes care that the 200 samples are available for both input ports, <literal>invalue1</literal> and <literal>invalue2</literal>. It then calls the <literal>calculateBlock</literal> method and tells it to calculate 200 samples and expects that it will generate exactly 200 samples of outvalue output. Synchronous streaming is the fastest and most easy-to-use variant of streaming, and it makes sense for most modules.</para>
<para>If, on the other hand, you think of a MIDI stream (that comes from a keyboard), things are different. The module wouldn't be able to guarantee that exactly the number of requested samples can be generated by <literal>calculateBlock;</literal> if the scheduler requests, <quote>Please give me 40 events,</quote> how could it do that when the person playing the keyboard isn't playing fast enough? For now, you have our synchronous streams; I'll talk more about the alternative model later.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>interface definitions</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>interface definitions</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>interface definitions</tertiary></indexterm></para>
</section>
<section id="ch14lev2sec3">
<title>Step 2&mdash;Pass That Definition Through <literal>mcopidl</literal></title>
<para>If<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>mcopidl</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>mcopidl</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>mcopidl</tertiary></indexterm><indexterm><primary>mcopidl</primary></indexterm> you have put all that into a file called example_add.idl, you can invoke <literal>mcopidl</literal>:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
$ <emphasis role="strong">mcopidl -I$KDEDIR/include/arts example_add.idl</emphasis>
</programlisting>
</informalexample>
<para>The <literal>-I</literal> flag adds a path to look for includes. If you don't have KDEDIR set to the position where KDE 2.0 is installed, you may have to use something explicit like the following, instead:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
-I/usr/local/kde-2.0/include/arts
</programlisting>
</informalexample>
<para>The IDL compiler now creates example_add.cc and example_add.h, which will be used later to implement and access the new <literal>Example_ADD</literal> module.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>mcopidl</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>mcopidl</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>mcopidl</tertiary></indexterm><indexterm><primary>mcopidl</primary></indexterm></para>
</section>
<section id="ch14lev2sec4">
<title>Step 3&mdash;Write an Implementation for the Interfaces You've Declared</title>
<para><link linkend="ch14list02">Listing 14.2</link> shows how to implement adding the sound.</para>
<example role="codelisting" label="14.2" id="ch14list02">
<title>Implementing the <literal>Example_ADD</literal> Interface<indexterm><primary>listings</primary><secondary>Example_ADD interface</secondary></indexterm></title>
<programlisting linenumbering="numbered">
 1: // example_add_impl.cc
 2:
 3: #include "example_add.h"
 4: #include "stdsynthmodule.h"
 5:
 6: class Example_ADD_impl
 7:               :public Example_ADD_skel, Arts::StdSynthModule
 8: {
 9: public:
10:     void calculateBlock(unsigned long samples)
11:     {
12:         unsigned long i;
13:         for(i=0;i &lt; samples;i++)
14:             result[i] = invalue1[i] + invalue2[i];
15:     }
16: };
17:
18: REGISTER_IMPLEMENTATION(Example_ADD_impl);<indexterm><primary>listings</primary><secondary>Example_ADD interface</secondary></indexterm>
</programlisting>
</example>
<para>As you can see, you derive from the skeleton class for the interface (which was generated by the <literal>mcopidl</literal> compiler). You also include the corresponding example_add.h (line 3). The other class you derive from is <literal>StdSynthModule</literal> because this contains some empty implementations of <literal>SynthModule</literal> methods that you often don't need to override.</para>
<para>Finally, consider the <literal>calculateBlock</literal> <indexterm><primary>calculateBlock function</primary></indexterm><indexterm><primary>functions</primary><secondary>calculateBlock</secondary></indexterm>method (line 10). This gets called whenever the module should process a block of audio data. The <literal>samples</literal> parameter tells the function how many samples to process. It is guaranteed that they are available at the corresponding pointers.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>interface implementation</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>interface implementation</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>interface implementation</tertiary></indexterm></para>
<para>Invisible to you (generated from the <literal>mcopidl</literal> compiler), the streams have become the following declarations in the <literal>Example_ADD_skel</literal> class (you inherit from that):</para>
<informalexample>
<programlisting linenumbering="unnumbered">
// variables for streams
float *invalue1;                  // incoming stream
float *invalue2;                  // incoming stream
float *result;                    // outgoing stream
</programlisting>
</informalexample>
<para>So the task of <literal>calculateBlock</literal> is easy:</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem><para>Take the data at the incoming streams and process them (use exactly <literal>samples</literal> values).</para></listitem>
<listitem><para>Write the output to the outgoing streams (also exactly <literal>samples</literal>). You must fill them; if you don't have anything to write (for instance, because you are getting that data from the Internet, and the Internet isn't fast enough to give you enough data), write 0.0 at least.</para></listitem>
<listitem><para>Do not modify the pointer itself. You may see this occasionally in some sources, but it isn't allowed any longer.<indexterm><primary>calculateBlock function</primary></indexterm><indexterm><primary>functions</primary><secondary>calculateBlock</secondary></indexterm></para></listitem>
</itemizedlist>
<para>As you see, the code is really easy to read.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>interface implementation</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>interface implementation</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>interface implementation</tertiary></indexterm></para>
</section>
<section id="ch14lev2sec5">
<title>Step 4&mdash;Register That Implementation with <literal>REGISTER_IMPLEMENTATION</literal></title>
<para>Finally<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>REGISTER_IMPLEMENTATION</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>REGISTER_IMPLEMENTATION</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>REGISTER_IMPLEMENTATION</tertiary></indexterm><indexterm><primary>REGISTER_IMPLEMENTATION</primary></indexterm><indexterm><primary>registering</primary><secondary>interface implementations</secondary><tertiary>REGISTER_IMPLEMENTATION</tertiary></indexterm>, <literal>REGISTER_IMPLEMENTATION</literal> is used to tell the MCOP object system that you have implemented an interface (you see this in the source under step 3). This has the following background: the objects you implement should be usable from programs that don't even know that such objects exists. For instance, <literal>artsbuilder</literal> will be a program that visually connects objects to larger graphs. Of course, it makes sense that your <literal>Example_ADD</literal> implementation can be used from <literal>artsbuilder</literal>, without <literal>artsbuilder</literal> knowing much about it.</para>
<para>Thus, <literal>artsbuilder</literal> can't simply call a constructor (because you would need to link <literal>artsbuilder</literal> to the class you just wrote and have a .h-file with the class definition, and so on). Instead, the <literal>REGISTER_IMPLEMENTATION</literal> macro defines a class that knows how to create one of your <literal>Example_ADD</literal> objects. If you then put only this in a shared library, the component can be used as a plug-in by applications that don't know anything about it.</para>
<para>This also means that you shouldn't need to have a header file in most cases, because MCOP provides ways to create an <literal>Example_ADD</literal> implementation without knowing that an <literal>Example_ADD_impl</literal> class exists.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>REGISTER_IMPLEMENTATION</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>REGISTER_IMPLEMENTATION</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>REGISTER_IMPLEMENTATION</tertiary></indexterm><indexterm><primary>REGISTER_IMPLEMENTATION</primary></indexterm><indexterm><primary>registering</primary><secondary>interface implementations</secondary><tertiary>REGISTER_IMPLEMENTATION</tertiary></indexterm></para>
</section>
<section id="ch14lev2sec6">
<title>Step 5&mdash;Maybe Write a .mcopclass File</title>
<para>I'll<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>.mcopclass files</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>.mcopclass files</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>.mcopclass files</tertiary></indexterm><indexterm><primary>mcopclass files</primary></indexterm><indexterm><primary>files</primary><secondary>.mcopclass files</secondary></indexterm> talk about this in the section <quote>Using the Effect.</quote> If you compile this in a libtool shared library libexample_add.la, you could write something like this in a file $KDEDIR/lib/Example_ADD.mcopclass:</para>
<para>That way, the MCOP dynamic library-loading mechanism would know that whenever you want to create an <literal>Example_ADD</literal> implementation, it could load the library. You'll do this later.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>.mcopclass files</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>.mcopclass files</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>.mcopclass files</tertiary></indexterm><indexterm><primary>mcopclass files</primary></indexterm><indexterm><primary>files</primary><secondary>.mcopclass files</secondary></indexterm></para>
</section>
<section id="ch14lev2sec7">
<title>How to Use the New Module</title>
<para>So<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>Example_ADD module</primary></indexterm> you've written a module (<link linkend="ch14list03">Listing 14.3</link>). Now, how do you use it?</para>
<example role="codelisting" label="14.3" id="ch14list03">
<title>Using an <literal>Example_ADD</literal> Module<indexterm><primary>listings</primary><secondary>Example_ADD module</secondary></indexterm></title>
<programlisting linenumbering="numbered">
 1: // example_add_test.cc
 2:
 3: #include "connect.h"
 4: #include "example_add.h"
 5: #include "artsflow.h"
 6:
 7: using namespace Arts;
 8:
 9: void main()
10: {
11:     // create a MCOP dispatcher (always do this)
12:     Dispatcher dispatcher;
13:
14:     Synth_FREQUENCY freq1, freq2;    // some objects
15:     Synth_WAVE_SIN sin1, sin2;
16:     Synth_MUL mul;
17:     Example_ADD add;
18:     Synth_PLAY play;
19:
20:     // setup a 440Hz sin and connect it to the add
21:     setValue(freq1,440.0);
22:     connect(freq1,sin1);
23:     connect(sin1,add,"invalue1");
24:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>Example_ADD module</primary></indexterm>
25:     // setup a 880Hz sin and connect it to the add
26:     setValue(freq2,880.0);
27:     connect(freq2,sin2);
28:     connect(sin2,add,"invalue2");
29:
30:     // multiply everything with 0.5 (=> no clipping)
31:     connect(add,result,mul,invalue1);
32:     setValue(mul,invalue2,0.5);
33:
34:     // connect the output to the play module
35:     connect(mul,play,"invalue_left");
36:     connect(mul,play,"invalue_right");
37:
38:     // start all modules
39:     freq1.start(); freq2.start(); sin1.start();
40:     sin2.start(); mul.start(); add.start(); play.start();
41:
42:     dispatcher.run();
43: }<indexterm><primary>listings</primary><secondary>Example_ADD module</secondary></indexterm>
</programlisting>
</example>
<para>You can compile it (with some tweaking if you have no <literal>KDEDIR</literal> set) with the following:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
$ <emphasis role="strong">gcc -o example_add_test example_add_test.cc example_add.cc</emphasis>
<emphasis role="strong">example_add_impl.cc -I$KDEDIR/include/arts</emphasis>
<emphasis role="strong">-L$KDEDIR/lib -lmcop -lartsflow_idl -lartsflow -ldl</emphasis>
</programlisting>
</informalexample>
<para>As you'll hear, it adds the sound just nicely. The resulting graph used here looks like <link linkend="ch14fig04">Figure 14.4</link>:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>modules, writing</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>modules, writing</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>writing</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>objects</primary><secondary>MCOP-aware, creating</secondary><tertiary>Example_ADD module</tertiary></indexterm><indexterm><primary>Example_ADD module</primary></indexterm></para>
<figure label="14.4" id="ch14fig04">
<title>Flow graph for <link linkend="ch14list03">Listing 14.3</link>.</title>
<mediaobject>
<imageobject>
<imagedata fileref="graphics/14fig04.gif" format="GIF"/>
</imageobject>
</mediaobject>
</figure>
</section>
</section>
<section id="ch14lev1sec3">
<title>MCOP</title>
<para>MCOP <indexterm><primary>multimedia</primary><secondary>MCOP</secondary></indexterm><indexterm><primary>MCOP</primary></indexterm>does a number of things for you. What probably impacts the way you work with multimedia objects most is the network transparency every MCOP object gets. You can interact in the same way with MCOP objects whether they are executed in the same process, in a different process on the same computer, or on a different computer.</para>
<para>In any case, MCOP objects are more than just C++ objects. So now I'll describe the details you need to know when using MCOP.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary></indexterm><indexterm><primary>MCOP</primary></indexterm></para>
<section id="ch14lev2sec8">
<title>The IDL Language</title>
<para>The <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>IDL syntax</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary></indexterm><indexterm><primary>IDL</primary></indexterm><indexterm><primary>IDL</primary><secondary>(interface definition language)</secondary></indexterm><indexterm><primary>interface definition language</primary><see>IDL</see></indexterm>interface definition language (IDL) serves one purpose: defining which interfaces certain objects offer. In contrast to <quote>normal</quote> C++ classes you define when programming C++ applications, all interfaces you define in MCOP IDL are supposed to be network transparent.</para>
<para>For that reason, it is not possible, for example, to simply make a function in an interface that returns a void-pointer. The same is valid for parameters. Also, you can't simply say, <quote>Well, this function takes a block of data of 1024 bytes,</quote> because depending on what you put into that block, the different byte order on different machines would make your interface not work correctly across the network.</para>
<para>So what definitions can you actually put into your IDL files?</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem><para><literal>#include</literal> statements that include other .idl files</para></listitem>
<listitem><para>Custom data types that are either</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem><para>Enumerations&mdash;Such as <literal>enum</literal> in C/C++</para></listitem>
<listitem><para>Structs&mdash;Such as <literal>struct</literal> in C/C++</para></listitem></itemizedlist>
</listitem>
<listitem><para>Interfaces, which may inherit other interfaces and contain the following:</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem><para>Methods that work with some well-known types</para></listitem>
<listitem><para>Streams, such as audio streams, event streams, or byte streams</para></listitem>
<listitem><para>Attributes<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>IDL syntax</tertiary></indexterm><indexterm><primary>IDL</primary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary></indexterm></para></listitem>
</itemizedlist></listitem></itemizedlist>
<para>Let's start with <literal>includes</literal>. They look like<indexterm><primary>IDL</primary><secondary>#include statements</secondary></indexterm><indexterm><primary>#include statements</primary></indexterm><indexterm><primary>statements</primary><secondary>#include</secondary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary><tertiary>#include statements</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
#include &lt;artsflow.idl>
</programlisting>
</informalexample>
<para>and will be searched in all paths you gave to <literal>mcopidl</literal> with the <literal>-I</literal> option. Their purpose is to ensure that <literal>mcopidl</literal> knows each type (and can decide if, for instance, <literal>User</literal> is an interface, a structure, or an enumeration value). Including files will generate a corresponding <literal>#include</literal> in the generated C++ source. That means if example_add.idl includes artsflow.idl, example_add.h will also include artsflow.h.<indexterm><primary>IDL</primary><secondary>#include statements</secondary></indexterm><indexterm><primary>#include statements</primary></indexterm><indexterm><primary>statements</primary><secondary>#include</secondary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary><tertiary>#include statements</tertiary></indexterm></para>
<para>Then, <indexterm><primary>IDL</primary></indexterm>there are the capabilities to define custom data types. The easiest are <emphasis>enumeration values</emphasis> (with the same syntax as in C++), for instance (taken from core.idl):</para>
<informalexample>
<programlisting linenumbering="unnumbered">
enum MethodType { methodOneway = 1, methodTwoway = 2 };
</programlisting>
</informalexample>
<para>Also, very similar to the C++ syntax are <literal>struct</literal>s, such as<indexterm><primary>IDL</primary><secondary>structs</secondary></indexterm><indexterm><primary>structs</primary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary><tertiary>structs</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
struct User {
    string name, password;
    long uid;
    sequence&lt;string> nicknames; // variable size of nicks
};
</programlisting>
</informalexample>
<para>The<indexterm><primary>IDL</primary><secondary>data types</secondary></indexterm><indexterm><primary>data types (IDL)</primary></indexterm><indexterm><primary>types (IDL)</primary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary><tertiary>data types</tertiary></indexterm> simple types you can use are <literal>long</literal>, <literal>string</literal>, <literal>float</literal>, <literal>byte</literal>, <literal>boolean</literal>, and it is also possible to write <literal>sequence&lt;</literal><emphasis><literal>sometype</literal></emphasis><literal>></literal> to get a variable size sequence (which roughly corresponds to arrays/pointers in C++).</para>
<para>All type concepts are there only to make defining interfaces with methods and attributes possible in a reasonable way. As I said earlier, because any MCOP interface should be network transparent, MCOP must know what types you pass around and how to deal with them.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>IDL syntax</tertiary></indexterm></para>
<para>So here is how you do <emphasis>interfaces</emphasis>, first of all, with simple <emphasis>methods</emphasis><emphasis role="strong">:</emphasis><indexterm><primary>IDL</primary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface HelloWorld /* : here you could inherit */ {
    void hello(User toWhichUser, boolean friendly);
};
</programlisting>
</informalexample>
<para>As you see, you can pass structures to methods, the same as you can pass normal values. The same is true for the return code. It is also possible to pass object references (simply by specifying the name of an interface as return code or parameter). You can also have <literal>oneway</literal> methods, which provide send-and-forget behavior. However, note that calling a <literal>oneway</literal> method returns immediately, so you can't rely on the fact that the method is done when your code goes on. Here is a <literal>oneway</literal> method:<indexterm><primary>IDL</primary><secondary>methods</secondary></indexterm><indexterm><primary>methods (IDL)</primary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary><tertiary>methods</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
oneway void play(string filename); // send-and-forget
</programlisting>
</informalexample>
<para>Finally, there are <emphasis>attributes</emphasis>, which are declared as follows:<indexterm><primary>IDL</primary><secondary>attributes</secondary></indexterm><indexterm><primary>attributes (IDL)</primary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary><tertiary>attributes</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface Window {
    attribute long width, height, x, y;
    readonly attribute handle;
};
</programlisting>
</informalexample>
<para>Here you see that there are two types of attributes: those that can be read and written and those that are read-only. It makes sense that for an X11 window, for instance, the window handle can only be read, whereas the position and size could be modified by writing the attribute. Here is a look at the C++ code necessary to read/write attributes:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>IDL syntax</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
Window w;
w.x(10);   // writing (that &raquo;means&laquo; w.x = 10)
w.y(10);

// reading
cout &lt;&lt; "moved window " &lt;&lt; w.handle()&lt;&lt; " to "
     &lt;&lt; " pos " &lt;&lt; w.x() &lt;&lt; ", " &lt;&lt; w.y() &lt;&lt; endl;<indexterm><primary>IDL</primary><secondary>attributes</secondary></indexterm><indexterm><primary>attributes (IDL)</primary></indexterm>
</programlisting>
</informalexample>
<para>Now to the last part&mdash;the most important part, <emphasis>streams</emphasis>. The syntax for defining streams is<indexterm><primary>IDL</primary><secondary>streams</secondary><tertiary>defining</tertiary></indexterm><indexterm><primary>streams</primary><secondary>defining</secondary></indexterm><indexterm><primary>defining</primary><secondary>streams</secondary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary><tertiary>stream definitions</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
[ async ] in/out [ multi ] <emphasis>type</emphasis> stream <emphasis>name</emphasis> [ , <emphasis>name</emphasis> &hellip;];
</programlisting>
</informalexample>
<para><link linkend="ch14table01">Table 14.1</link> explains the stream's syntax.</para>
<table id="ch14table01" frame="topbot" label="14.1">
<title>Defining Streams in the.idl File</title>
<tgroup cols="2" align="left" colsep="0" rowsep="0">
<colspec colname="c1"/>
<colspec colname="c2"/>
<thead valign="top">
<row rowsep="1">
<entry><emphasis>Element</emphasis></entry>
<entry><emphasis>Description</emphasis></entry></row>
</thead>
<tbody valign="top">
<row>
<entry><literal>[ async ]</literal><indexterm><primary>async element (IDL)</primary></indexterm></entry>
<entry>Used to make a stream asynchronous. Asynchronous streams are those that transfer data only sometimes&mdash;not continuously&mdash;or that can't always produce data when you ask them to. More about that in the section <link linkend="ch14lev2sec14"><quote>Synchronous versus Asynchronous Streams.</quote></link></entry>
</row>
<row>
<entry><literal>in/out</literal><indexterm><primary>in/out element (IDL)</primary></indexterm></entry>
<entry>This gives the direction of the stream: incoming or outgoing</entry></row>
<row>
<entry><literal>[ multi ]</literal><indexterm><primary>multi element (IDL)</primary></indexterm></entry>
<entry>Used to say that this stream can accept multiple connections. For instance, if you have a mixer that can mix any number of audio signals, it would have a multi-input stream. There are no multi-out streams.</entry>
</row>
<row>
<entry><emphasis><literal>type</literal></emphasis></entry>
<entry>The data type that gets streamed. Audio is a way to say <literal>float</literal>, because all audio data will really be passed around as floats. Not all data types are allowed for streaming</entry></row>
<row>
<entry><literal>stream</literal></entry>
<entry>This means that you want to declare a stream.</entry></row>
<row>
<entry><emphasis><literal>name</literal></emphasis></entry>
<entry>The name of the stream. You can define many streams at once (if they have the same parameters) by giving more than one name here.<indexterm><primary>IDL</primary><secondary>streams</secondary><tertiary>defining</tertiary></indexterm><indexterm><primary>streams</primary><secondary>defining</secondary></indexterm><indexterm><primary>defining</primary><secondary>streams</secondary></indexterm></entry>
</row>
</tbody>
</tgroup>
</table>
<para>The normal streaming type you'll mostly use is <literal>audio</literal> (and this is a synchronous stream). Internally, this audio data is represented as <literal>float</literal>. Mostly, you'll define streams as shown next:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>IDL syntax</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface Synth_MUL : SynthModule {
    in audio stream invalue1,invalue2;
    out audio stream outvalue;
};
</programlisting>
</informalexample>
<para>If you inherit from an interface that already has streams, it may even happen that you don't need to add anything at all; for instance:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface StereoFFTScope : StereoEffect {
    readonly attribute sequence&lt;float> scope;
};
</programlisting>
</informalexample>
<para>In this example, appropriate streams are inherited from <literal>StereoEffect</literal>.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>IDL syntax</tertiary></indexterm><indexterm><primary>IDL</primary><secondary>streams</secondary><tertiary>defining</tertiary></indexterm><indexterm><primary>streams</primary><secondary>defining</secondary></indexterm><indexterm><primary>defining</primary><secondary>streams</secondary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL syntax</secondary><tertiary>stream definitions</tertiary></indexterm></para>
</section>
<section id="ch14lev2sec9">
<title>Invoking the IDL Compiler</title>
<para>The <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>IDL compiler</tertiary></indexterm><indexterm><primary>IDL</primary><secondary>compiler</secondary><tertiary>invoking</tertiary></indexterm><indexterm><primary>compiler (IDL)</primary><secondary>invoking</secondary></indexterm><indexterm><primary>invoking</primary><secondary>IDL compiler</secondary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL compiler</secondary><tertiary>invoking</tertiary></indexterm>IDL compiler is easy to use. It is called as shown next:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
mcopidl <emphasis>flags</emphasis> file.idl
</programlisting>
</informalexample>
<para><emphasis><literal>flags</literal></emphasis> specify the flags used when processing the IDL file. The IDL compiler then creates file.cc and file.h, which contain the necessary classes to enable network transparency, scheduling, and other gimmicks. With the <literal>-I</literal> flag, you can add include paths to search. If you want to add multiple paths, use <literal>-I</literal> more than once.</para>
<para>If you want to integrate an <literal>mcopidl</literal> call into the <literal>make</literal> process, the following (which can be used to build the example mentioned previously) could be some inspiration:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
MCOPIDL=mcopidl
MCOPINC=-I$(KDEDIR)/include/arts
MCOPLIB=-L$(KDEDIR)/lib -lartsflow -lartsflow_idl -lmcop -ldl
SRCS=example_add.cc example_add_test.cc example_add_impl.cc

all: example_add_test

example_add_test: $(SRCS)
    gcc -o example_add_test $(MCOPINC) $(SRCS) $(MCOPLIB)

example_add.cc: example_add.idl
    $(MCOPIDL) $(MCOPINC) example_add.idl

example_add.h: example_add.cc
</programlisting>
</informalexample>
<para>Of course, you'll need to adapt that a bit. For Automake, for instance, it's a good idea to put example_add.cc/example_add.h in the metasources section.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>IDL compiler</tertiary></indexterm><indexterm><primary>IDL</primary><secondary>compiler</secondary><tertiary>invoking</tertiary></indexterm><indexterm><primary>compiler (IDL)</primary><secondary>invoking</secondary></indexterm><indexterm><primary>invoking</primary><secondary>IDL compiler</secondary></indexterm><indexterm><primary>MCOP</primary><secondary>IDL compiler</secondary><tertiary>invoking</tertiary></indexterm></para>
</section>
<section id="ch14lev2sec10">
<title>Reference Counting</title>
<para>When <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>reference counting</tertiary></indexterm><indexterm><primary>references</primary><secondary>counting</secondary></indexterm><indexterm><primary>counting</primary><secondary>references</secondary></indexterm><indexterm><primary>objects</primary><secondary>references</secondary><tertiary>counting</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>reference counting</secondary></indexterm>you write</para>
<informalexample>
<programlisting linenumbering="unnumbered">
Synth_PLAY p;
</programlisting>
</informalexample>
<para>in your source code, you create a reference to a <literal>Synth_PLAY</literal> object, not a <literal>Synth_PLAY</literal> object itself. What happens is that as soon as you actually try to use <literal>p</literal>, an implementation is created for you. That happens, for instance, as soon as you write</para>
<informalexample>
<programlisting linenumbering="unnumbered">
p.start();
</programlisting>
</informalexample>
<para>Because this is only a reference, writing things such as</para>
<informalexample>
<programlisting linenumbering="unnumbered">
Synth_PLAY q = p;
</programlisting>
</informalexample>
<para>doesn't create a second <literal>Synth_PLAY</literal> object, but only makes <literal>q</literal> point to the same object as <literal>p</literal>. MCOP keeps track of how many references point to a certain object. If this count goes to zero, the object is freed.</para>
<para>Thus, you never need to care about pointers when using MCOP objects, and you also don't need the <literal>new</literal> or <literal>delete</literal> operators.</para>
<para>One of the nice things is that this reference counting works even in the distributed case. If you have a server process that hands out an object reference to a client process (for instance, as return code), the object on the server will not be freed, unless the client no longer holds references to the object.</para>
<para>MCOP is so smart that it recognizes client crashes. That means if you (as server) create an object specifically for one client and that client doesn't need it anymore (or crashes), the object will be removed.</para>
<para>Of course, this works only if you don't hold any references to the object yourself inside the server.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>reference counting</tertiary></indexterm><indexterm><primary>references</primary><secondary>counting</secondary></indexterm><indexterm><primary>counting</primary><secondary>references</secondary></indexterm><indexterm><primary>objects</primary><secondary>references</secondary><tertiary>counting</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>reference counting</secondary></indexterm></para>
</section>
<section id="ch14lev2sec11">
<title>Initial Object References</title>
<para>When <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>initial object reference</tertiary></indexterm><indexterm><primary>references</primary><secondary>initial object references</secondary></indexterm><indexterm><primary>initial object references</primary></indexterm><indexterm><primary>objects</primary><secondary>references</secondary><tertiary>initial object references</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>initial object references</secondary></indexterm>you have everything&mdash;interface definitions, implementations, and a server (for instance a soundserver), how does the client start talking to the interface?</para>
<para>For this problem, the MCOP object manager (which you can access with <literal>ObjectManager::the()</literal>) provides these functions:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
class ObjectManager {    // from objectmanager.h
    [&hellip;]
    bool addGlobalReference(Object *object,
         std::string name);
    std::string getGlobalReference(std::string name);
    void removeGlobalReferences();
};
</programlisting>
</informalexample>
<para>With <literal>addGlobalReference</literal><indexterm><primary>addGlobalReference function</primary></indexterm><indexterm><primary>functions</primary><secondary>addGlobalReference</secondary></indexterm>, you can say, <quote>I have implemented an object, and everybody can use it under the name&hellip;.</quote> For instance, the aRts soundserver <literal>artsd</literal> makes a <literal>SimpleSoundServer</literal> interface available under the name <literal>Arts_SimpleSoundServer</literal>.</para>
<para>With <literal>getGlobalReference</literal><indexterm><primary>getGlobalReference function</primary></indexterm><indexterm><primary>functions</primary><secondary>getGlobalReference</secondary></indexterm>, you can get a string that you can convert into an object reference again. And finally, <literal>removeGlobalReferences</literal><indexterm><primary>removeGlobalReferences function</primary></indexterm><indexterm><primary>functions</primary><secondary>removeGlobalReferences</secondary></indexterm> can be used to remove all global references you have added.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>initial object reference</tertiary></indexterm><indexterm><primary>references</primary><secondary>initial object references</secondary></indexterm><indexterm><primary>initial object references</primary></indexterm><indexterm><primary>objects</primary><secondary>references</secondary><tertiary>initial object references</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>initial object references</secondary></indexterm></para>
<para>These global references are shared among all MCOP-aware processes. There are currently two strategies of doing so. Either in the /tmp/mcop-<emphasis>username</emphasis> directory or on the X11 server. Whichever one is used depends on the user's configuration.</para>
<para>The following is a very useful shortcut to getting global references:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
SimpleSoundServer server(
    Reference("global:Arts_SimpleSoundServer"));

if(server.isNull()) { /* error handling */ }
</programlisting>
</informalexample>
<para>After these lines, you can use the <literal>SimpleSoundServer</literal> as if it were a local object. For instance, call</para>
<informalexample>
<programlisting linenumbering="unnumbered">
server.play("/usr/local/share/pling.wav");
</programlisting>
</informalexample>
<para>and your requests will be sent to the <literal>artsd</literal> soundserver.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>initial object reference</tertiary></indexterm><indexterm><primary>references</primary><secondary>initial object references</secondary></indexterm><indexterm><primary>initial object references</primary></indexterm><indexterm><primary>objects</primary><secondary>references</secondary><tertiary>initial object references</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>initial object references</secondary></indexterm></para>
</section>
<section id="ch14lev2sec12">
<title>Accessing Streams</title>
<para>Most<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>stream access</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>stream access</secondary></indexterm><indexterm><primary>streams</primary><secondary>accessing</secondary></indexterm><indexterm><primary>acessing</primary><secondary>streams</secondary></indexterm> of the time when you're dealing with streams, you'll write <literal>calculateBlock</literal> implementations. And most of the time when you write those, they'll access only synchronous audio streams. In that case, the only thing you need to do is to process all samples you read from the streams&mdash;for instance, in one <literal>for</literal> loop like the following (from our <literal>Example_ADD</literal> from the beginning):</para>
<informalexample>
<programlisting linenumbering="unnumbered">
void calculateBlock(unsigned long samples)
{
    unsigned long i;
    for(i=0;i != cycles;i++)
        outvalue[i] = invalue1[i] + invalue2[i];
}
</programlisting>
</informalexample>
<para>As you see, the streams have been mapped to simple <literal>float *</literal> pointers by the <literal>mcopidl</literal> compiler. In your <literal>calculateBlock</literal> function<indexterm><primary>calculateBlock function</primary></indexterm><indexterm><primary>functions</primary><secondary>calculateBlock</secondary></indexterm></para>
<itemizedlist mark="bullet" spacing="compact">
<listitem><para>The scheduler will supply you with <literal>samples</literal> input values.</para></listitem>
<listitem><para>You must fill all output streams exactly with <literal>samples</literal> values (if you have nothing to write, write 0.0 values instead).</para></listitem>
<listitem><para>You may not modify the pointer itself.</para></listitem>
</itemizedlist>
<para>For multiple input streams (which are declared with the <literal>multi</literal> keyword in the IDL), the mapping isn't <literal>float *</literal>, but <literal>float **</literal>. When <literal>calculateBlock</literal> is called, the <literal>float **</literal> will point to an array of <literal>float *</literal> buffers, and the end is marked by a null pointer. So you can use a code fragment like that to process multi-input streams:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
void calculateBlock(unsigned long samples)
{
    float *inp;
    for(int sig=0;(inp = invalue[sig]) != 0;sig++)
    {
        /* process input from inp here */
    }
}
</programlisting>
</informalexample>
<para>Here, the same rules as those for single streams apply, with the addition that</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem><para>Your code should handle the case in which no input at all is connected to the multi-input stream properly, as well.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>stream access</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>stream access</secondary></indexterm><indexterm><primary>streams</primary><secondary>accessing</secondary></indexterm><indexterm><primary>acessing</primary><secondary>streams</secondary></indexterm></para></listitem>
</itemizedlist>
</section>
<section id="ch14lev2sec13">
<title>Module Initialization</title>
<para>Module <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>module initialization</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>module initialization</secondary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>initializing</secondary></indexterm><indexterm><primary>initializing</primary><secondary>MCOP modules</secondary></indexterm>initialization and deinitialization happens through a number of ways. They are chronologically listed here. As most modules don't need all the initialization facilities provided by the <literal>SynthModule</literal> interface, a small class has been written that implements all of them as empty methods. Thus, you can rewrite only the parts you need while leaving, for instance, <literal>streamStart()</literal> untouched/empty. It is called <literal>StdSynthModule</literal>, and it gets used through inheritance, such as the following (example from synth_add_impl.cc):</para>
<informalexample>
<programlisting linenumbering="unnumbered">
#include "stdsynthmodule.h"
using namespace Arts;
class Synth_ADD_impl :public Synth_ADD_skel, StdSynthModule
[&hellip;]
</programlisting>
</informalexample>
<section id="ch14lev3sec1">
<title>C++ Constructor</title>
<para>First, <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>module initialization</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>module initialization</secondary><tertiary>C++ constructor</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>initializing</secondary><tertiary>C++ constructor</tertiary></indexterm><indexterm><primary>initializing</primary><secondary>MCOP modules</secondary><tertiary>C++ constructor</tertiary></indexterm>, there is the traditional C++ <emphasis>constructor</emphasis>. You can use this as always&mdash;to allocate resources that your module will need in any case, to initialize members with certain values, and so on.</para>
</section>
<section id="ch14lev3sec2">
<title>Attributes</title>
<para>Later <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>module initialization</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>module initialization</secondary><tertiary>attributes</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>initializing</secondary><tertiary>attributes</tertiary></indexterm><indexterm><primary>initializing</primary><secondary>MCOP modules</secondary><tertiary>attributes</tertiary></indexterm>on, the user of the module (or some automatic mechanism, such as the flowgraph based initialization <literal>artsbuilder</literal> will do) will set the attributes. You should accept all changes there in any order. For instance, if your module relies on a <emphasis><literal>filename</literal></emphasis> attribute and a <emphasis><literal>format</literal></emphasis> attribute, it is a valid usage of your module, first to set the <emphasis><literal>filename</literal></emphasis>, then the <emphasis><literal>format</literal></emphasis>, and then choose another <emphasis><literal>filename</literal></emphasis> again. Also, querying your attributes at that phase should return sensible values. <literal>artsbuilder</literal> will provide some RAD-like component development, so your modules should be configurable gracefully and fully over the attributes (and not over special initialization functions).</para>
<para>Setting and getting attributes is valid at any point in time between the constructor and destructor, especially while the module is running.</para>
</section>
<section id="ch14lev3sec3">
<title><literal>streamInit</literal></title>
<para>After<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>module initialization</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>module initialization</secondary><tertiary>streamInit() function</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>initializing</secondary><tertiary>streamInit() function</tertiary></indexterm><indexterm><primary>initializing</primary><secondary>MCOP modules</secondary><tertiary>streamInit() function</tertiary></indexterm><indexterm><primary>streamInit() function</primary></indexterm><indexterm><primary>functions</primary><secondary>streamInit()</secondary></indexterm> all attributes have been set completely, the <literal>streamInit()</literal> function is called (before your module is started). In that function, you should do all that is necessary before actually starting. For some modules, a difference exists between that initialization phase and actually starting. For example, consider the sound card I/O. In the <literal>streamInit()</literal> function, it opens the sound card, sets the parameters, allocates the buffers, and prepares anything.</para>
</section>
<section id="ch14lev3sec4">
<title><literal>streamStart</literal></title>
<para>Finally <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>module initialization</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>module initialization</secondary><tertiary>streamStart() function</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>initializing</secondary><tertiary>streamStart() function</tertiary></indexterm><indexterm><primary>initializing</primary><secondary>MCOP modules</secondary><tertiary>streamStart() function</tertiary></indexterm><indexterm><primary>streamStart() function</primary></indexterm><indexterm><primary>functions</primary><secondary>streamStart()</secondary></indexterm>, in <literal>streamStart()</literal> only the last bit is done. In the case of our sounddriver, only the <literal>IOManager</literal> registration is done, which actually causes writing. The idea is that initialize should do all operations that may take longer (for instance, allocating and filling a 16KB buffer may, under ugly circumstances, take longer because it needs to get swapped in first). On the other hand, registering an I/O watch should be fast. After <literal>streamStart()</literal> has been called, the module will be ready to go. The <literal>calculateBlock</literal> function gets called as soon as the scheduler thinks it is necessary.</para>
</section>
<section id="ch14lev3sec5">
<title><literal>streamEnd</literal></title>
<para>Finally <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>module initialization</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>module initialization</secondary><tertiary>streamEnd() function</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>initializing</secondary><tertiary>streamEnd() function</tertiary></indexterm><indexterm><primary>initializing</primary><secondary>MCOP modules</secondary><tertiary>streamEnd() function</tertiary></indexterm><indexterm><primary>streamEnd() function</primary></indexterm><indexterm><primary>functions</primary><secondary>streamEnd()</secondary></indexterm>, when your module gets stopped, <literal>streamEnd()</literal> is called. That function should undo all effects caused by <literal>streamStart()</literal> and <literal>streamInit()</literal>. Note that the scheduler may decide not to free your module immediately, but to fill it with new attributes and use it again for some other task. Therefore, don't do things in the constructor/destructor that really belong to <literal>streamInit()</literal>/<literal>streamEnd()</literal>.</para>
</section>
<section id="ch14lev3sec6">
<title>C++ Destructor</title>
<para>Eventually, <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>module initialization</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>module initialization</secondary><tertiary>C++ destructor</tertiary></indexterm><indexterm><primary>modules (MCOP)</primary><secondary>initializing</secondary><tertiary>C++ destructor</tertiary></indexterm><indexterm><primary>initializing</primary><secondary>MCOP modules</secondary><tertiary>C++ destructor</tertiary></indexterm>when everything is done, the C++ destructor gets called, where you can free things you have set up in the constructor.</para>
</section>
</section>
<section id="ch14lev2sec14">
<title>Synchronous Versus Asynchronous Streams</title>
<para>Synchronous<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>synchronous versus asynchronous streams</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>synchronous versus asynchronous streams</secondary></indexterm><indexterm><primary>synchronous streams</primary></indexterm><indexterm><primary>asynchronous streams</primary></indexterm><indexterm><primary>streams</primary><secondary>synchronous versus asynchronous</secondary></indexterm> streams are used whenever samples are happening at periodic time intervals and your module can, when given a certain amount of input, guarantee producing the same amount of output. For most modules, such as those that add signals or process them with other calculations, this should be no problem. However, modules that depend on external resources, such as the piano player that generates the MIDI events or the network connection that supplies the data, can't make such guarantees.</para>
<para>The same can be true for consuming data as well. Modules that depend on the external network connection to receive everything they send can make only limited guarantees that the data you feed into them really disappears.</para>
<para>Thus, asynchronous streams offer a greater amount of control. They send around the data in packets. The basic idea is this: the sender sends packets, and the receiver receives packets and acknowledges when they have been processed completely (see <link linkend="ch14fig05">Figure 14.5</link>).</para>
<figure label="14.5" id="ch14fig05">
<title>How asynchronous streaming works.</title>
<mediaobject>
<imageobject>
<imagedata fileref="graphics/14fig05.gif" format="GIF"/>
</imageobject>
</mediaobject>
</figure>
<para>There are now two basic forms of behavior for a sender: <emphasis>push delivery</emphasis> and <emphasis>pull delivery</emphasis>.</para>
<para>Push delivery occurs when the sender only casually generates a data packet. This is true, for instance, for a MIDI receiver connected to an external MIDI keyboard. Events are generated only when the human player plays some notes. The MIDI sender can assume that in this case it can simply put things into packets and the receiver should be able to process what it gets in time.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>synchronous versus asynchronous streams</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>synchronous versus asynchronous streams</secondary></indexterm><indexterm><primary>synchronous streams</primary></indexterm><indexterm><primary>asynchronous streams</primary></indexterm><indexterm><primary>streams</primary><secondary>synchronous versus asynchronous</secondary></indexterm></para>
<para>The API for doing so is simple: suppose the stream is called <literal>outdata</literal>, and the datatype that is being sent is <literal>byte</literal>:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
DataPacket&lt;mcopbyte> *packet;
packet = outdata.allocPacket(15);  // alloc 15 bytes
strcpy((char *)packet->contents,"Hello World");
packet->size = strlen("Hello World");
packet->send();
</programlisting>
</informalexample>
<para>As you can see, you can shrink the size of the data sent after allocating the packet. The purpose of this is that you can use system functions such as <literal>read()</literal> or <literal>write()</literal>, for instance, directly on the buffer inside the data packet and, after that, decide how many of these bytes should be sent. Sending data packets with zero length frees them immediately.</para>
<para>Now to the other case, that happens if you want to send a sample stream of bytes asynchronously from inside an application (such as the game Quake) to the soundserver. There you want synchronization with the receiver; that is, you want to send packets only as fast as the receiver processes them.</para>
<para>Push delivery works like that: you get calls from the scheduler when you should produce packets. To initialize the process, you ask the scheduler to prepoll <emphasis>x</emphasis> packets with the size <emphasis>y</emphasis>. Then it will ask you <emphasis>x</emphasis> times to fill such a packet. They are sent to the receiver(s). When they have processed them, they will come back, and you will be asked to refill the packets.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>synchronous versus asynchronous streams</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>synchronous versus asynchronous streams</secondary></indexterm><indexterm><primary>synchronous streams</primary></indexterm><indexterm><primary>asynchronous streams</primary></indexterm><indexterm><primary>streams</primary><secondary>synchronous versus asynchronous</secondary></indexterm></para>
<para>Starting the process happens with something like the following:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
outdata.setPull(8, 1024);
</programlisting>
</informalexample>
<para>After that call, you'll be asked eight times to refill a packet of 1024 bytes. These packets will be sent to the receiver(s). After they have processed the packets, you'll get new requests to refill packets. Thus, the only thing you need to get this working is a refill routine:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
void request_outdata(DataPacket&lt;mcopbyte> *packet)
{
    packet->size = 1024;
    for(int i = 0;i &lt; 1024; i++)
        packet->contents[i] = (mcopbyte)'A';
    packet->send();
}
</programlisting>
</informalexample>
<para>and that is it.</para>
<para>For the receiver, things are even simpler. As soon as it gets packets, the <literal>process_</literal><emphasis><literal>streamname</literal></emphasis> function is called, and it should call <literal>packet->process</literal> as soon as it is really done processing a packet. A process function for a byte stream that prints everything to <literal>stdout</literal> would be:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
void process_indata(DataPacket&lt;mcopbyte> *inpacket)
{
    char *instring = (char *)inpacket->contents;
    for(int i=0;i&lt;inpacket->size;i++)
        putchar(instring[i]);
    inpacket->processed();
}
</programlisting>
</informalexample>
<para>The receiver may delay the process called and do it some time after the <literal>process_</literal><emphasis><literal>streamname</literal></emphasis> function, as well, if that is when the packet is really processed.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>synchronous versus asynchronous streams</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>synchronous versus asynchronous streams</secondary></indexterm><indexterm><primary>synchronous streams</primary></indexterm><indexterm><primary>asynchronous streams</primary></indexterm><indexterm><primary>streams</primary><secondary>synchronous versus asynchronous</secondary></indexterm></para>
</section>
<section id="ch14lev2sec15">
<title>Connecting Objects</title>
<para>Objects <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>object connections</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>object connections</secondary></indexterm><indexterm><primary>objects</primary><secondary>connecting</secondary></indexterm><indexterm><primary>connecting</primary><secondary>objects</secondary></indexterm><indexterm><primary>connect() function</primary></indexterm><indexterm><primary>functions</primary><secondary>connect()</secondary></indexterm>can be connected with the <literal>connect()</literal> function, which is declared in connect.h. The concept of default ports plays a certain role here. The standard syntax for connect is</para>
<informalexample>
<programlisting linenumbering="unnumbered">
connect(from_object, from_port, to_object, to_port);
</programlisting>
</informalexample>
<para>However, this can be simplified when the objects have suitable default ports. For instance, all objects with only one incoming/outgoing stream default to using them in connect, so that the following connect operations are the same:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
Synth_FREQUENCY freq;
Synth_WAVE_SIN wave;
connect(freq,"pos",wave,"pos");
connect(freq,wave);
</programlisting>
</informalexample>
<para>For modules with more than one port, default ports usually work as well (for example, <literal>Synth_PLAY</literal> with <literal>invalue_left</literal> and <literal>invalue_right</literal> defaults to using both as default ports). You can find exactly which modules they are for with the help of the IDL files.</para>
<para>Under the <literal>node()</literal>/<literal>_node()</literal> accessor of every <literal>SynthModule</literal> is a more complete API for modules. There, the <literal>connect</literal>/<literal>disconnect</literal>/<literal>start</literal>/<literal>stop</literal> functions are defined. At the time of this writing, MCOP is still a work in progress. Probably, disconnect and stop will be available soon under <literal>stop()</literal> like <literal>start()</literal> and <literal>disconnect()</literal> similar to <literal>connect()</literal> with default ports and anything else.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>object connections</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>object connections</secondary></indexterm><indexterm><primary>objects</primary><secondary>connecting</secondary></indexterm><indexterm><primary>connecting</primary><secondary>objects</secondary></indexterm><indexterm><primary>connect() function</primary></indexterm><indexterm><primary>functions</primary><secondary>connect()</secondary></indexterm></para>
</section>
</section>
<section id="ch14lev1sec4">
<title>Standard Interfaces</title>
<para>The <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm>whole point of a middleware such as MCOP is to make objects talk to each other to fulfill their task. The following are some of the interfaces that are the most important to get started.</para>
<section id="ch14lev2sec16">
<title>The <literal>SimpleSoundServer</literal> Interface</title>
<para>The<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>SimpleSoundServer</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>SimpleSoundServer</secondary></indexterm><indexterm><primary>SimpleSoundServer interface</primary></indexterm><indexterm><primary>sound</primary><secondary>SimpleSoundServer interface</secondary></indexterm> <literal>SimpleSoundServer</literal> interface is the interface that the KDE soundserver <literal>artsd</literal> provides when running. To connect to it, you can simply use the following lines:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
SimpleSoundServer server(
    Reference("global:Arts_SimpleSoundServer"));

if(server.isNull()) { /* error handling */ }
</programlisting>
</informalexample>
<para>Make sure not to access functions of the server after you find out <literal>isNull()</literal> is true. So what does it offer? First, it offers the most basic command, playing some file (which may be .wav or any other format aRts can understand), with the simple method:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
long play(string filename);
</programlisting>
</informalexample>
<para>Therefore, in a few lines, you can write a client that plays wave files. If you already have a <literal>SimpleSoundServer</literal> called server, its just</para>
<informalexample>
<programlisting linenumbering="unnumbered">
server.play("/var/share/sounds/asound.wav");
</programlisting>
</informalexample>
<note role="normal">
<para>It is necessary here to pass a <emphasis>full path</emphasis>, because it is very likely that your program doesn't have the same working directory as <literal>artsd</literal>. Thus, calling play with an unqualified name will mostly fail. For instance, if you are in /var/share/sounds and <literal>artsd</literal> is in /home/kde2, if you write <literal>play</literal> (<literal>"asound.wav"</literal>), the server would try to play /home/kde2/asound.wav.</para>
</note>
<para>The <literal>play</literal> method returns a long value with an ID. If playing succeeded, you can use this to stop the sound again with<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>SimpleSoundServer</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>SimpleSoundServer</secondary></indexterm><indexterm><primary>SimpleSoundServer interface</primary></indexterm><indexterm><primary>sound</primary><secondary>SimpleSoundServer interface</secondary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
void stop(long ID);
</programlisting>
</informalexample>
<para>if not, the ID is 0.</para>
<para>Then, there is another set of methods to attach or detach streaming-sound sources, such as games that do their own sound mixing (Quake, for instance). They are called</para>
<informalexample>
<programlisting linenumbering="unnumbered">
void attach(ByteSoundProducer producer);
void detach(ByteSoundProducer producer);
</programlisting>
</informalexample>
<para>If you want to use these, the way to go is to implement a <literal>ByteSoundProducer</literal> object. This has an outgoing asynchronous byte stream, which can be used to send the data as signed 16-bit little endian stereo. Then, simply create such an object inside your process. For adapting Quake, the <literal>ByteSoundProducer</literal> object should be created inside the Quake process, and all audio output should be put into the data packets sent via the asynchronous streaming mechanism. Finally, a call to <literal>attach()</literal><indexterm><primary>attach function</primary></indexterm><indexterm><primary>functions</primary><secondary>attach</secondary></indexterm> with the object is enough to start streaming.</para>
<para>When you're done, call <indexterm><primary>detach function</primary></indexterm><indexterm><primary>functions</primary><secondary>detach</secondary></indexterm><literal>detach()</literal>. An example showing how to implement a <literal>ByteSoundProducer</literal> is in the kdelibs/arts/examples directory. But in most cases, a simpler way is possible. For porting games such as Quake, there is also the C API, which encapsulates the aRts functionality. Thus, there are routines similar to those needed to access the operating system audio drivers, like OSS (open sound system, the Linux sound drivers). These are called <literal>arts_open()</literal>, <literal>arts_write()</literal>, <literal>arts_close(),</literal> and so on, which, in turn, call the things that ought to happen in the background.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>SimpleSoundServer</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>SimpleSoundServer</secondary></indexterm><indexterm><primary>SimpleSoundServer interface</primary></indexterm><indexterm><primary>sound</primary><secondary>SimpleSoundServer interface</secondary></indexterm></para>
<para>Whether a layer will be written to simplify the usage of the streaming API for KDE 2.0 apps remains to be seen. If there is time to do a <literal>KAudioStream</literal>, which handles all attach/detach and packet production, it will go into some KDE library.</para>
<para>Finally, two functions are left. One is</para>
<informalexample>
<programlisting linenumbering="unnumbered">
object createObject(string name);
</programlisting>
</informalexample>
<para>It can be used to create an arbitrary object on the soundserver. Therefore, if you need an <literal>Example_ADD</literal> for some reason&mdash;and it shouldn't be running inside your process, but inside the soundserver process&mdash;a call looking like this:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
Example_ADD e = DynamicCast(server.createObject("Example_ADD"));
if(e.isNull()) { /* fail */ }
</programlisting>
</informalexample>
<para>should do the trick. As you see, you can easily cast <literal>Object</literal> to <literal>Example_ADD</literal> using <literal>DynamicCast</literal>.</para>
<para>Just a few words explaining why you may want to create something on the server. Imagine that you want to develop a 3D game, but you are missing 3D capabilities inside aRts, such as creating moving sound sources and things like that. Of course, you can render all that locally (inside the game process) and transfer the result via streaming to the soundserver. However, a latency penalty and a performance penalty are associated with that.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>SimpleSoundServer</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>SimpleSoundServer</secondary></indexterm><indexterm><primary>SimpleSoundServer interface</primary></indexterm><indexterm><primary>sound</primary><secondary>SimpleSoundServer interface</secondary></indexterm></para>
<para>The latency penalty is this: you need to do streaming in packets, which have a certain size. If you want to have no dropouts when your game doesn't get the CPU for a few milliseconds, you need to dimension these like four packets with 2048 bytes each, or something like that. Although the resulting total time needed to replay all packets of 47 milliseconds protects you from dropouts, it also means that after a player shoots, you'll have a 47-millisecond delay until the 3D sound system reacts. On the other hand, if your 3D sound system runs inside the server, the time to tell it <quote>player shoots now</quote> would normally be around 1 millisecond (because it is one <literal>oneway</literal> remote invocation). Thus, you can reduce the latency by 47 milliseconds by creating things server side.</para>
<para>The performance penalty, on the other hand, is clear. Putting all that stuff into packets and taking it out again takes CPU time. With very small latencies (small packets), you need more packets per second, and thus, the performance penalty increases. So for real-time applications such as games, running things server side is the most important.</para>
<para>Last but not least, let's take a look at effects. The server allows inserting effects between the downmixed signal of all clients and the output. That is possible with the attribute</para>
<informalexample>
<programlisting linenumbering="unnumbered">
readonly attribute StereoEffectStack outstack;
</programlisting>
</informalexample>
<para>As you see, you get a <literal>StereoEffectStack</literal>, for which the interface will be described soon. It can be used to add effects to the chain.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>SimpleSoundServer</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>SimpleSoundServer</secondary></indexterm><indexterm><primary>SimpleSoundServer interface</primary></indexterm><indexterm><primary>sound</primary><secondary>SimpleSoundServer interface</secondary></indexterm></para>
</section>
<section id="ch14lev2sec17">
<title>The KMedia2 Interfaces</title>
<para>KMedia2 <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>KMedia2</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>KMedia2</secondary></indexterm><indexterm><primary>KMedia2 interface</primary></indexterm>is nothing but a big remote control. It allows you to create objects that play some kind of media (such as .wavs, MP3s, but&mdash;at least from what the interfaces allow&mdash;also CDs or streams from URLs). This is achieved through one interface, called <literal>PlayObject</literal>, and it looks like the following:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface PlayObject : PlayObject_private {
    attribute string description;
    attribute poTime currentTime;
    readonly attribute poTime overallTime;
    readonly attribute poCapabilities capabilities;
    readonly attribute string mediaName;
    readonly attribute poState state;
    void play();
    void seek(poTime newTime);
    void pause();
};
</programlisting>
</informalexample>
<para>As you can see, this is enough for telling the object to play, pause, and seek anytime. After you have a <literal>PlayObject</literal>, you should have no difficulties dealing with it. There is something to be said about the <literal>poTime</literal> type, which is used to represent custom times. For instance, a mod player could count in patterns internally, while also doing calculations in seconds (which is more appropriate for the user to read). Thus, <literal>poTime</literal> allows you to define custom times, like this:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
struct poTime {
    long ms, seconds;  // -1 if undefined
    float custom;      // some custom time unit
                       // -1 if undefined
    string customUnit; // for instance "pattern"
};
</programlisting>
</informalexample>
<para><literal>PlayObjects</literal> are allowed to define either the <quote>normal</quote> time, the <quote>custom</quote> time, or both, just as they please. Also, seeking can be done only on the time type the <literal>PlayObject</literal> understands. (For example, if a mod player understands only patterns, you can seek only with patterns). Then there are capabilities, which can be used for the PlayObject to say, <quote>Well, I am a stream from an URL; you can't seek me at all.</quote> They look like this:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>KMedia2</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>KMedia2</secondary></indexterm><indexterm><primary>KMedia2 interface</primary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
enum poCapabilities { capSeek = 1, capPause = 2 };
</programlisting>
</informalexample>
<para>and finally the different states the <literal>PlayObject</literal> are:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
enum poState { posPlaying, posFinished, posPaused };
</programlisting>
</informalexample>
<para>Still, some part is missing. You not only need to know how to talk to <literal>PlayObjects</literal>, you need to know how to create them in the first place. For that, there is <literal>PlayObjectFactory</literal>, which looks like the following:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface PlayObjectFactory {
    PlayObject createPlayObject(string filename);
};
</programlisting>
</informalexample>
<para>That's it. You have a factory for creating <literal>PlayObjects</literal>, and you know that they will disappear automatically, as soon as you no longer reference them. And the last missing piece, <quote>Where do I get that <literal>PlayObjectFactory</literal> from?</quote> is simple: it's a global reference, and it is called <literal>Arts_PlayObjectFactory</literal>, so it works the same as with the <literal>SimpleSoundServer</literal> interface.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>KMedia2</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>KMedia2</secondary></indexterm><indexterm><primary>KMedia2 interface</primary></indexterm></para>
</section>
<section id="ch14lev2sec18">
<title>Stereo Effects/Effectstacks</title>
<para>Let's <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>StereoEffectStack</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>StereoEffectStack</secondary></indexterm><indexterm><primary>StereoEffectStack interface</primary></indexterm><indexterm><primary>sound</primary><secondary>StereoEffectStack interface</secondary></indexterm>take a closer look at another interface that is used to put effects in a chain. Basically, a <literal>StereoEffectStack</literal> looks like <link linkend="ch14fig06">Figure 14.6</link>:</para>
<figure label="14.6" id="ch14fig06">
<title>How a <literal>StereoEffectStack</literal> works.</title>
<mediaobject>
<imageobject>
<imagedata fileref="graphics/14fig06.gif" format="GIF"/>
</imageobject>
</mediaobject>
</figure>
<para>Each of the inserted effects should have the following interface:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface StereoEffect : SynthModule {
    default in audio stream inleft, inright;
    default out audio stream outleft, outright;
};
</programlisting>
</informalexample>
<para>and all normal <literal>StereoEffects</literal> derive from that. For example, two of them are <literal>StereoVolumeControl</literal> and <literal>StereoFFTScope</literal>. However, there is the <literal>StereoEffectStack</literal> interface itself, which looks like this:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface StereoEffectStack : StereoEffect {
    long insertTop(StereoEffect effect, string name);
    long insertBottom(StereoEffect effect, string name);

    void remove(long ID);
};
</programlisting>
</informalexample>
<para>As you can see, the <literal>StereoEffectStack</literal> is a <literal>StereoEffect</literal> itself, which means it has the same inleft, inright, outleft, and outright streams. Thus, you can set the inputs and outputs by connecting these. You can also insert effects at the top or at the bottom. If you have no effects, the inputs and outputs will simply get connected. Finally, you can remove effects again by ID.</para>
<para><literal>SimpleSoundServer</literal> provides you with a <literal>StereoEffectStack</literal> that is between the sound mixing and the output. Initially, it's empty. If you want effects, you can insert them into the stack, and if you want to remove them, that's also no issue.</para>
<para>And that is what you're going to do next.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>interfaces</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>interfaces</secondary><tertiary>StereoEffectStack</tertiary></indexterm><indexterm><primary>interfaces</primary><secondary>StereoEffectStack</secondary></indexterm><indexterm><primary>StereoEffectStack interface</primary></indexterm><indexterm><primary>sound</primary><secondary>StereoEffectStack interface</secondary></indexterm></para>
</section>
</section>
<section id="ch14lev1sec5">
<title>Implementing a <literal>StereoEffect</literal></title>
<para>After<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary></indexterm><indexterm><primary>sound</primary><secondary>multimedia</secondary></indexterm> you know how server-side object creation works, what stereo effects are, and how you can get them running, it would be a nice idea to actually do this in an example. Next, let's write something similar to <literal>Example_ADD</literal> that works like a <literal>StereoEffect</literal>.</para>
<para>First, what should it do? I thought it may be useful to emulate the standard options you have for balance at every amplifier: keep stereo as it is, keep left channel only, keep right channel only, reverse channels, and downmix stereo.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary></indexterm></para>
<section id="ch14lev2sec19">
<title>IDL Again</title>
<para>First, <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>IDL (interface definition language)</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>IDL (interface definition language)</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>IDL (interface definition language)</tertiary></indexterm>you need a representation for these states. That is done best using an enumeration value. So you get the following:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
#include &lt;artsflow.idl>
enum StereoBalanceState { sbThrough, sbLeftOnly,
              sbRightOnly, sbReverse, sbDownMix };
</programlisting>
</informalexample>
<para>Then, you need to make the interface derive from <literal>StereoEffect</literal>. Thus, no streams need to be declared at all because <literal>StereoEffect</literal> already does this. The interface looks as simple as this:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
interface StereoBalanceControl : Arts::StereoEffect {
    attribute StereoBalanceState balance;
};
</programlisting>
</informalexample>
<para>That's all. Put it into a file balance.idl and invoke <literal>mcopidl</literal>:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>IDL (interface definition language)</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>IDL (interface definition language)</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>IDL (interface definition language)</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
<emphasis role="strong">$ mcopidl -I$KDEDIR/include/arts balance.idl</emphasis>
</programlisting>
</informalexample>
</section>
<section id="ch14lev2sec20">
<title>The Code</title>
<para>Now<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>balance() function</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>balance() function</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>balance() function</tertiary></indexterm><indexterm><primary>balance() function</primary></indexterm><indexterm><primary>functions</primary><secondary>balance()</secondary></indexterm> to the implementation. First to the attribute stuff; these are mapped, as I mentioned previously, to two functions: one that changes the value and one that queries it. They are both called <literal>balance</literal>. One gets a parameter to set a new value, and one returns the old value. You should also have an internal variable to store the current state (it's called <literal>_balance</literal> here), and initialize this properly in the C++ constructor. Up to now, you have the following:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
#include "balance.h"
#include &lt;stdsynthmodule.h>

using namespace Arts;

class StereoBalanceControl_impl : public
    StereoBalanceControl_skel, StdSynthModule
{
private:
    StereoBalanceState _balance;
public:
    StereoBalanceControl_impl() : _balance(sbThrough) {}
    StereoBalanceState balance()    { return _balance; }
    void balance(StereoBalanceState b) { _balance = b; }
</programlisting>
</informalexample>
<para>The method that is still missing is <literal>calculateBlock</literal>. The most important thing to watch here is that the volume doesn't change through our balance control. That means, for downmix (mixing the left and right stereo channels to mono output), you shouldn't simply add everything, but divide by two. The other cases shouldn't be hard to handle. Here is the rest of the code, then, while you are left with some cases to write, too:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>balance() function</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>balance() function</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>balance() function</tertiary></indexterm><indexterm><primary>balance() function</primary></indexterm><indexterm><primary>functions</primary><secondary>balance()</secondary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
    void calculateBlock(unsigned long samples)
{
        unsigned long i;
        switch (_balance) {
            case sbThrough:
                for(i=0;i&lt;samples;i++)
                {
                    outleft[i] = inleft[i];
                    outright[i] = inright[i];
                }
                break;
            case sbDownMix:
                for(i=0;i&lt;samples;i++)
                {
                    float mix = (inleft[i]+inright[i])/2;
                    outleft[i] = mix;
                    outright[i] = mix;
                }
                break;
            /* exercise : implement the other cases */
        };
    }
};
REGISTER_IMPLEMENTATION(StereoBalanceControl_impl);<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>balance() function</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>balance() function</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>balance() function</tertiary></indexterm><indexterm><primary>balance() function</primary></indexterm><indexterm><primary>functions</primary><secondary>balance()</secondary></indexterm>
</programlisting>
</informalexample>
</section>
<section id="ch14lev2sec21">
<title>Using the Effect</title>
<para>Because you want to be able to load and use the effect inside the server, put it into a library that can be dynamically loaded. To do so, you'll also create a class definition called StereoBalanceControl.mcopclass, with the following content:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
Library=libstereobalancecontrol.la
</programlisting>
</informalexample>
<para>Because making is a bit difficult, the following is a makefile again, for making the whole module:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>makefile</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>makefile</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>makefile</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
LIBDIR=$(KDEDIR)/lib
CXX=libtool --mode=compile g++
CXXFLAGS=-I$(KDEDIR)/include/arts
LD=libtool --mode=link g++
LDFLAGS=-module -rpath $(LIBDIR) -L$(LIBDIR) -lartsflow \
                     -lartsflow_idl -lmcop -ldl
CP=libtool --mode=install cp
TARGET=libstereobalancecontrol.la
OBJS=balance.lo balance_impl.lo

all: $(TARGET)

install: $(TARGET)
    $(CP) $(TARGET) $(LIBDIR)
    $(CP) StereoBalanceControl.mcopclass $(LIBDIR)

libstereobalancecontrol.la: $(OBJS)
    $(LD) -o $(TARGET) $(LDFLAGS) $(OBJS)

balance_impl.lo: balance_impl.cc
    $(CXX) $(CXXFLAGS) -c balance_impl.cc

balance.lo: balance.cc
    $(CXX) $(CXXFLAGS) -c balance.cc

balance.cc: balance.idl
    mcopidl $(CXXFLAGS) balance.idl
</programlisting>
</informalexample>
<para>Thus, compilation and installation are simply done with<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>makefile</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>makefile</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>makefile</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
$ <emphasis role="strong">make</emphasis>
$ <emphasis role="strong">make install</emphasis>
</programlisting>
</informalexample>
<para>although you may need to do the last as root (that is, <literal>su root -c 'make install'</literal>). Okay, now you have installed an effect, which should be dynamically loadable into the server. To try it out, <link linkend="ch14list04">Listing 14.4</link> is a test program:<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>running on server</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>running on server</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>running on server</tertiary></indexterm></para>
<example role="codelisting" label="14.4" id="ch14list04">
<title>Running StereoBalanceControl on the Server<indexterm><primary>listings</primary><secondary>StereoBalanceControl, running on server</secondary></indexterm></title>
<programlisting linenumbering="numbered">
 1: #include "balance.h"
 2: #include &lt;soundserver.h>
 3: #include &lt;stdio.h>
 4:
 5: using namespace Arts;
 6:
 7: void fail(char *why) { printf("%s\n",why); exit(1); }
 8:
 9: int main(int argc, char **argv)
 10: {
11:     if(argc != 2) fail("use two arguments");
12:
13:     Dispatcher dispatcher;
14:     SimpleSoundServer server(Reference("global:Arts_SimpleSoundServer"));
15:     if(server.isNull()) fail("can't connect server");
16:
17:     StereoBalanceControl bcontrol;
18:     bcontrol = DynamicCast(server.createObject("StereoBalanceControl"));
19:     if(bcontrol.isNull()) fail("can't create object");
20:
21:     if(strcmp(argv[1],"downmix") == 0)
22:         bcontrol.balance(sbDownMix);
23:     if(strcmp(argv[1],"through") == 0)
24:         bcontrol.balance(sbThrough);
25:     /* add the others possibilities, if you like */
26:     bcontrol.start();
27:
28:     StereoEffectStack effectstack = server.outstack();
29:     long id=effectstack.insertBottom(bcontrol,"Balance");
30:     printf("type return to quit\n"); getchar();
31:     effectstack.remove(id);
32:     return 0;
33: }<indexterm><primary>listings</primary><secondary>StereoBalanceControl, running on server</secondary></indexterm>
</programlisting>
</example>
<para>Finally, to get that running, do the following: First compile<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>running on server</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>running on server</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>running on server</tertiary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
$ <emphasis role="strong">g++ -o setbalance setbalance.cc  -I$KDEDIR/include/arts</emphasis>
<emphasis role="strong">-L$KDEDIR/lib -lsoundserver_idl -lartsflow -lartsflow_idl</emphasis>
<emphasis role="strong">-lstereobalancecontrol -lmcop -ldl</emphasis>
</programlisting>
</informalexample>
<para>That is everything in one line. Then call <literal>ldconfig</literal> as root (to get your freshly installed library registered):</para>
<informalexample>
<programlisting linenumbering="unnumbered">
$ <emphasis role="strong">su root -c 'ldconfig'</emphasis>
</programlisting>
</informalexample>
<para>Make sure that something is running on the soundserver (for instance, listen to an MP3 or .wav), and then type</para>
<informalexample>
<programlisting linenumbering="unnumbered">
$ <emphasis role="strong">setbalance downmix</emphasis>
</programlisting>
</informalexample>
<para>That should downmix the stuff that is played. As you can see, running objects server side is really easy through MCOP. Imagine what the overhead would be like if you needed to transfer all data from the server to the setbalance program, which would do the calculations, and you had to transfer everything back again.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>StereoBalanceControl sample program</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>running on server</tertiary></indexterm><indexterm><primary>StereoBalanceControl sample program</primary><secondary>running on server</secondary></indexterm><indexterm><primary>sound</primary><secondary>StereoBalanceControl sample program</secondary><tertiary>running on server</tertiary></indexterm></para>
</section>
</section>
<section id="ch14lev1sec6">
<title>KDE Multimedia Besides MCOP</title>
<para>As you have seen in the previous sections, MCOP is the basis of many multimedia things, and it allows you many freedoms. However, in the very common situation in which you just want to notify the user by playing a sample, there are simpler interfaces. Also, not all media types have been integrated so far; I'll try to briefly address the other possibilities that are available.</para>
<section id="ch14lev2sec22">
<title>KNotify API and <literal>KAudioPlayer</literal></title>
<para>There<indexterm><primary>multimedia</primary><secondary>KNotify API</secondary></indexterm><indexterm><primary>KNotify API</primary></indexterm><indexterm><primary>multimedia</primary><secondary>KAudioPlayer class</secondary></indexterm><indexterm><primary>KAudioPlayer class</primary></indexterm><indexterm><primary>classes</primary><secondary>KAudioPlayer</secondary></indexterm><indexterm><primary>sound</primary><secondary>KAudioPlayer class</secondary></indexterm> are two very simple ways to get a sound played in a KDE application. Of course, they use the aRts soundserver. However, they are available in the kdecore library and require no extra libraries. Thus, they are the most convenient forms of doing audio.</para>
<para>First is the <literal>KAudioPlayer</literal> class (declared in kaudioplayer.h). It is supposed to play a sound file once, and without feedback. It works like this:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
KAudioPlayer::play("/var/share/foo.wav");
</programlisting>
</informalexample>
<para>You see, it's simple. It also has the capability to use signals and slots to play files. That looks like the following:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
KAudioPlayer player("/var/share/foo.wav");
connect(&amp;btn,SIGNAL(clicked()),&amp;player,SLOT(play()));
</programlisting>
</informalexample>
<para>However, for some applications, configurable sound events would be nicer for the user. For instance, how can you know which sound somebody prefers when getting new mail? How can you know whether the user prefers a sound at all, and not simply a message onscreen?</para>
<para>Of course, every application could write the configuration for that itself. There is a better solution, however. Using the KNotify API, you create an eventsrc, where you describe your events. The user can reconfigure them later. The following is a sample eventsrc file (which needs to be installed in $KDEDIR/share/apps/keventtest/eventsrc):<indexterm><primary>multimedia</primary><secondary>KNotify API</secondary></indexterm><indexterm><primary>KNotify API</primary></indexterm><indexterm><primary>multimedia</primary><secondary>KAudioPlayer class</secondary></indexterm><indexterm><primary>KAudioPlayer class</primary></indexterm><indexterm><primary>classes</primary><secondary>KAudioPlayer</secondary></indexterm><indexterm><primary>sound</primary><secondary>KAudioPlayer class</secondary></indexterm></para>
<informalexample>
<programlisting linenumbering="unnumbered">
[!Global!]
Name=keventtest
Comment=Event Test Program
[newmail]
Name=New Mail
Comment=Occurs when you've got new mail
default_sound=/var/samples/samples/011.WAV
default_presentation=1
</programlisting>
</informalexample>
<para>And here is how to use it:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
KNotifyClient::event("newmail");
</programlisting>
</informalexample>
<para>The user now can disable this event, specify another sound file, or make it a message box instead of a sound file in the KDE control center. This is why you should prefer events to simply playing files in most cases.<indexterm><primary>multimedia</primary><secondary>KNotify API</secondary></indexterm><indexterm><primary>KNotify API</primary></indexterm><indexterm><primary>multimedia</primary><secondary>KAudioPlayer class</secondary></indexterm><indexterm><primary>KAudioPlayer class</primary></indexterm><indexterm><primary>classes</primary><secondary>KAudioPlayer</secondary></indexterm><indexterm><primary>sound</primary><secondary>KAudioPlayer class</secondary></indexterm></para>
</section>
<section id="ch14lev2sec23">
<title>LibKMid</title>
<para>If <indexterm><primary>multimedia</primary><secondary>LibKMid</secondary></indexterm><indexterm><primary>LibKMid</primary></indexterm><indexterm><primary>sound</primary><secondary>LibKMid</secondary></indexterm><indexterm><primary>MIDI</primary><secondary>LibKMid</secondary></indexterm>you want to have background MIDI music for games and similar things, there is LibKMid. It comes with the capability to read and play MIDI files. It also does fork itself, so you don't have to care that it keeps running while you do longer calculations.</para>
<para>I think that the following example will show the most essential things you need to know:</para>
<informalexample>
<programlisting linenumbering="unnumbered">
KMidSimpleAPI::kMidInit();
KMidSimpleAPI::kMidLoad("fancymusic.mid");
KMidSimpleAPI::kMidPlay();

sleep(30);  /* of course, you can do anything here */

KMidSimpleAPI::kMidStop();
KmidSimpleAPI::kMidDestruct();
</programlisting>
</informalexample>
<para>The initialization and loading are done with one function call each. After you have triggered playing (<literal>kMidPlay</literal>), you can do anything you like (for instance, for a visual application, return into the event loop). LibKMid will care that the MIDI file keeps running in the background.<indexterm><primary>multimedia</primary><secondary>LibKMid</secondary></indexterm><indexterm><primary>LibKMid</primary></indexterm><indexterm><primary>sound</primary><secondary>LibKMid</secondary></indexterm><indexterm><primary>MIDI</primary><secondary>LibKMid</secondary></indexterm><indexterm><primary>music</primary><see>sound</see></indexterm></para>
</section>
<section id="ch14lev2sec24">
<title>aKtion</title>
<para>Finally <indexterm><primary>multimedia</primary><secondary>aKtion</secondary></indexterm><indexterm><primary>aKtion</primary></indexterm>, a hint about aKtion. Video isn't integrated in MCOP yet. It probably will be someday. However, that doesn't mean that there is no way to play videos. Based on the xanim decoders, aKtion is able to play most common video formats. It can, of course, be used as standalone application.</para>
<para>However, it is also available as an embeddable part via the KParts technology. That makes it suitable for using it inside Konqueror, but also inside your application.<indexterm><primary>multimedia</primary><secondary>aKtion</secondary></indexterm><indexterm><primary>aKtion</primary></indexterm></para>
</section>
</section>
<section id="ch14lev1sec7">
<title>The Future of MCOP</title>
<para>MCOP <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>future of</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>future of</secondary></indexterm><indexterm><primary>future technology</primary><secondary>MCOP</secondary></indexterm>is new. This means that not every feature that should be available is available already. Here I'll try to outline the most important plans and show you where implementations are currently missing, so that you get an orientation of how the big picture will look when it is completed.</para>
<section id="ch14lev2sec25">
<title>Composition/RAD</title>
<para><literal>artsbuilder</literal>, <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>future of</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>future of</secondary><tertiary>composition/RAD</tertiary></indexterm><indexterm><primary>future technology</primary><secondary>MCOP</secondary><tertiary>composition/RAD</tertiary></indexterm><indexterm><primary>artsbuilder</primary></indexterm>which existed already for arts-0.3.4, needs to be ported to the KDE 2.0 aRts/MCOP technology still. It will allow building more complex objects out of simpler ones visually. For instance, if you have a delay module, you can easily built a reverb filter by using a few delay modules and adding them together (with some feedback). All this should be available in an easy-to-use visual builder (just like the arts-0.3.4 <literal>artsbuilder</literal>).</para>
<para>This needs some internal tweaking in MCOP so that you can implement complex modules in terms of easy modules transparently.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>future of</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>future of</secondary><tertiary>composition/RAD</tertiary></indexterm><indexterm><primary>future technology</primary><secondary>MCOP</secondary><tertiary>composition/RAD</tertiary></indexterm><indexterm><primary>artsbuilder</primary></indexterm></para>
</section>
<section id="ch14lev2sec26">
<title>GUIs</title>
<para>As <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>future of</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>future of</secondary><tertiary>GUIs</tertiary></indexterm><indexterm><primary>future technology</primary><secondary>MCOP</secondary><tertiary>GUIs</tertiary></indexterm><indexterm><primary>GUIs</primary><secondary>MCOP and</secondary></indexterm>you have seen, writing plug-ins for the soundserver is easy. Using them in other software as wave editors, hard disk recorders, and sequencing software such as Brahms (which is a CuBase clone for KDE) should be no problem, as well. What is missing is the capability to make GUIs for those plug-ins easily. Maybe, if you are reading this, this is already fixed.</para>
<para>It would be nice to have the <literal>StereoBalanceControl</literal> object you wrote available in artscontrol, KWave, Brahms, and other software, with a nice control panel to configure the balance to be actually used.</para>
<para>With the modularity I mentioned previously, the GUI building should be as flexible. It should be possible for somebody without any programming skills to create a reverb effect out of some delays and give it a nice GUI. Maybe GUIs should also be toolkit independent, as a side effect of that.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>future of</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>future of</secondary><tertiary>GUIs</tertiary></indexterm><indexterm><primary>future technology</primary><secondary>MCOP</secondary><tertiary>GUIs</tertiary></indexterm><indexterm><primary>GUIs</primary><secondary>MCOP and</secondary></indexterm></para>
</section>
<section id="ch14lev2sec27">
<title>Scripting</title>
<para>Programming<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>future of</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>future of</secondary><tertiary>scripting</tertiary></indexterm><indexterm><primary>future technology</primary><secondary>MCOP</secondary><tertiary>scripting</tertiary></indexterm><indexterm><primary>scripting</primary><secondary>MCOP</secondary></indexterm> signal flow in C++ is nice. But you may not always want to wait for your compiler to achieve a small task. If components could be scripted by JavaScript and/or KScript, another powerful tool would be added to the multimedia capabilities. You could do whole presentations with amazing video and audio combinations just as scripts. You could implement even more complex modules than with <literal>artsbuilder</literal>, without knowing C++ at all, or needing your compiler, or anything else.</para>
</section>
<section id="ch14lev2sec28">
<title>More Media Types</title>
<para>Finally <indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>future of</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>future of</secondary><tertiary>media types</tertiary></indexterm><indexterm><primary>future technology</primary><secondary>MCOP</secondary><tertiary>media types</tertiary></indexterm> the most important point right now is that sound is what you can currently do reasonably with aRts/MCOP technology. The possibilities would be greatly enhanced if MIDI and video were modular, just like sound. Video codecs and effects, modular MIDI processing, and modular sound would give users and developers the ultimate unified multimedia API for solving complex problems easily.</para>
<para>It will happen; and if I know KDE development, it won't take too long.<indexterm><primary>multimedia</primary><secondary>MCOP</secondary><tertiary>future of</tertiary></indexterm><indexterm><primary>MCOP</primary><secondary>future of</secondary><tertiary>media types</tertiary></indexterm><indexterm><primary>future technology</primary><secondary>MCOP</secondary><tertiary>media types</tertiary></indexterm></para>
</section>
</section>
<section id="ch14lev1sec8">
<title>Summary</title>
<para>What KDE 2.0 offers in the multimedia section is more than a few convenient classes. It is way of doing things. After reading this chapter, you should have an impression of how the parts interact and how MCOP helps in solving multimedia tasks.</para>
<para>Next, I'll summarize, in four levels, the key features that were mentioned in this chapter.</para>
<para>The highest level is the theoretical point of view. All multimedia tasks are somehow flow graphs using small modules.</para>
<para>Then, you can look at it from an application level. I am writing an application. What do I need to know? Talking to interfaces, creating and connecting modules, connecting to the soundserver, and similar things are relevant here.</para>
<para>One level below, interfaces themselves become interesting. One side is which standard interfaces does aRts/MCOP provide, and how are they useful? Some important interfaces are the <literal>SimpleSoundServer</literal> interface, the <literal>StereoEffect/StereoEffectStack</literal>, and KMedia2 with the <literal>PlayObjects</literal>.</para>
<para>The other side is the interface definition language itself, IDL, and its interaction with mcopidl. Interfaces in MCOP are oriented toward multimedia. Specifying streams directly in the interface, and thus allowing MCOP to deal with them, is one of the important ways to get the flow graph concept really done nicely.</para>
<para>Finally, the lowest level is implementing the interfaces. After writing the <literal>StereoBalanceControl</literal> implementation, you should have an impression of how writing the small modules that implement the interfaces in C++ works. The relevant aspects of streaming here are the initialization, the <literal>calculateBlock</literal> function when you do synchronous streaming, or their equivalents in the case of asynchronous streaming.</para>
</section>
<section id="ch14lev1sec9">
<title>Exercises</title>
<qandaset defaultlabel="number">
<qandaentry>
<question id="ch14que01">
<para><link linkend="ch14ans01">Implement a beep sound similar to the stereo beep at the beginning, but with a variable frequency. Make the frequency change very slowly between 220.0 and 660.0 to achieve a siren effect. If you want to keep the source simple, don't do different things for the left and right channels.</link></para>
</question>
</qandaentry>
<qandaentry>
<question id="ch14que02">
<para><link linkend="ch14ans02">Complete the missing cases in the <literal>StereoBalanceControl</literal> module above.</link></para>
</question>
</qandaentry>
<qandaentry>
<question id="ch14que03">
<para><link linkend="ch14ans03">If you want a challenge now&mdash;something really tricky&mdash;I've got something for you. Otherwise, you can safely ignore this. Here it is: Rewrite the stereo beep example in a way that the beeps are spinning in circles from the left channel to the right channel and back to the left channel. Have fun!</link></para>
</question>
</qandaentry>
</qandaset>
</section>
</chapter>
